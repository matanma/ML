{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from pandas_summary import DataFrameSummary\n",
    "import pandas_profiling as pp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.debugger import set_trace\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "%matplotlib inline\n",
    "from IPython.core.debugger import set_trace #!!TODO: remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "train_set_df = pd.read_csv(\"train_set.csv\", parse_dates = ['timestamp'], date_parser = parse )\n",
    "test_set_df = pd.read_csv(\"test_set.csv\", parse_dates = ['timestamp'], date_parser = parse )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_raw = train_set_df.label\n",
    "train_set_grouped_by_patients = train_set_df.groupby('patient_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial examination of the raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = DataFrameSummary(train_set_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.ProfileReport(train_set_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distribution of labels over the raw data is approx: 68% - class 0, 20% - class 1, and 12% - class 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmp = plt.hist(y_train_raw, bins =  [0,1,2,3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0]/sum(tmp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The labels are attributes of patients, not of specific samples (all patient's samples have the same label )** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_0 = y_train_raw==0\n",
    "inds_1 = y_train_raw==1\n",
    "inds_2 = y_train_raw==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patiends_0_set = set(train_set_df[inds_0]['patient_id'])\n",
    "patiends_1_set = set(train_set_df[inds_1]['patient_id'])\n",
    "patiends_2_set = set(train_set_df[inds_2]['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patiends_0_set.intersection(patiends_1_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patiends_0_set.intersection(patiends_2_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patiends_1_set.intersection(patiends_2_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each patient's data consists of 3 time series. The number of samples in these series across the patients varies between 1 (single sample) and nearly 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_samples_sizes = [len(pdf[1]) for pdf in train_set_grouped_by_patients]\n",
    "plt.hist(train_set_samples_sizes, bins = np.arange(0,100));\n",
    "plt.title(\"Train set - #samples per patient\");\n",
    "plt.xlabel(\"#samples\");\n",
    "plt.ylabel(\"#patients\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The test set has similar distribution, which means, in particular, that we'll need to predict labels of series consisting of a single time sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_samples_sizes = [len(pdf[1]) for pdf in test_set_df.groupby('patient_id')]\n",
    "plt.hist(test_set_samples_sizes, bins = np.arange(0,100));\n",
    "plt.title(\"Test set - #samples per patient\");\n",
    "plt.xlabel(\"#samples\");\n",
    "plt.ylabel(\"#patients\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the timedelta between each sample is 50ms, the corresponding series durations vary between 0 sec (single sample) to nearly 5 sec (100 samples)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_samples_durations = [(pdf[1]['timestamp'].iloc[-1] - pdf[1]['timestamp'].iloc[0]).total_seconds() for pdf in train_set_grouped_by_patients]\n",
    "plt.hist(train_set_samples_durations, density=True, bins = np.arange(0, 5, 0.1));\n",
    "plt.title(\"Patients timeseries duration (seconds) \");\n",
    "plt.xlabel(\"series duration\");\n",
    "plt.ylabel(\"density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some series are inconsistent in terms of their sizes and durations. Below we see some series with 5 samples (corresponding to 0.2 sec), whose duration is less than 0.2 sec** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_set_samples_durations)[np.array(train_set_samples_sizes)==5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's find the one with duration 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax((np.array(train_set_samples_sizes)==5) & (np.array(train_set_samples_durations) < 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_grouped_by_patients.get_group(18186)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seems that the samples behave properly (they are not constant), so it seems like a bug in the timestamps registration. We can either remove such samples as outliers or fix their timestamps based on their number of samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's another example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_grouped_by_patients.get_group(np.argmax((np.array(train_set_samples_sizes)==5) & (np.array(train_set_samples_durations) == 0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inconsistent_indices = ((np.array(train_set_samples_sizes) - 1)*0.05 - np.array(train_set_samples_durations)) > 0.00001\n",
    "print(f\"there are {all_inconsistent_indices.sum()} patients whose time series duration is shorter than their 50ms*#samples\")\n",
    "all_inconsistent_indices_upper = ((np.array(train_set_samples_sizes) - 1)*0.05 - np.array(train_set_samples_durations)) < - 0.00001\n",
    "print(f\"there are {all_inconsistent_indices_upper.sum()} patients whose time series duration is longer than their 50ms*#samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set_samples_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inconsistent_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_samples_sizes[-1], train_set_samples_durations[-1], train_set_samples_sizes[-1]*0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_grouped_by_patients.get_group(25706-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c(train_set_samples_durations[0]/0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "29*0.05 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TimeSeries - acquaintance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the class is a property of the patient, let's examine the per-patient classes. The classes proportions remain the same as when counting the raw samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_classes_df = train_set_grouped_by_patients.nth(0)['label']\n",
    "print(f\"there are {len(patients_classes_df)} patients altogether\")\n",
    "patients_classes_df.hist(bins=[0,1,2,3], density=True);\n",
    "y_per_patient = patients_classes_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a patient timeseries, optionally with the regressed approximations superimposed (this comes later on)\n",
    "def plot_patient_lines(patient_id, extracted_features = None, plot_ax = None):\n",
    "    if plot_ax is None:\n",
    "        fig_p, ax_p = plt.subplots()\n",
    "    else:\n",
    "        ax_p = plot_ax\n",
    "    \n",
    "    curr_patient_df = train_set_grouped_by_patients.get_group(patient_id)\n",
    "    first_timestamp = curr_patient_df['timestamp'].iloc[0]\n",
    "    curr_patient_df.index = list(map(lambda ts : (ts -first_timestamp).total_seconds()*1000, curr_patient_df['timestamp']))\n",
    "    curr_patient_df[['measurement_x', 'measurement_y', 'measurement_z']].plot(ax=ax_p)\n",
    "    if extracted_features is not None:\n",
    "        coefs = extracted_features.reshape(3,3).T\n",
    "        x_regr = curr_patient_df.index.values\n",
    "        x_mat = np.array([np.ones(len(x_regr)), x_regr, x_regr*x_regr ]).T\n",
    "        curr_patient_df_approx = pd.DataFrame( x_mat.dot(coefs), \n",
    "                                              columns = ['measurement_x_regr', 'measurement_y_regr', 'measurement_z_regr'], \n",
    "                                                index = curr_patient_df.index\n",
    "                                             )\n",
    "        curr_patient_df_approx[['measurement_x_regr', 'measurement_y_regr', 'measurement_z_regr']].plot(ax=ax_p, style='--')\n",
    "    ax_p.set_title(f\"Patient {patient_id} timeseries\")\n",
    "    ax_p.set_xlabel(\"time from first timestamp (ms)\")\n",
    "    return len(curr_patient_df), ax_p\n",
    "\n",
    "#Choose n_patient random patients of class patient_class\n",
    "def choose_rand_patients( n_patients, patient_class ):\n",
    "    if patient_class not in [0,1,2]:\n",
    "        raise Exception(\"Specified patient_class is not in existing classes set: [0,1,2]\")\n",
    "    patient_class_inds = y_per_patient==patient_class\n",
    "    num_class_patients = sum(patient_class_inds)\n",
    "    if n_patients > num_class_patients:\n",
    "        raise Exception(\"Specified num samples {} exceeds the number of patients of requested class {}\".format(n_patients, patient_class))\n",
    "    selected_class_inds = random.sample(range(0, num_class_patients), n_patients)\n",
    "    selected_patients = patients_classes_df[patient_class_inds].index[selected_class_inds]\n",
    "    return selected_patients.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's view some timeseries of each of the classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patients_ids = choose_rand_patients(1,1)\n",
    "\n",
    "plot_patient_lines(patients_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typical and less typical classes timeseries**\n",
    "\n",
    "Based on sproradic visual inspection of the timelines. Left images are the more typical, right ones seems less typical to each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_figsize = plt.rcParams['figure.figsize'] \n",
    "plt.rcParams['figure.figsize'] = [12, 3]\n",
    "fig_0, (ax_0_typical, ax_0_non_typical) = plt.subplots(1,2)\n",
    "plot_patient_lines(20072, plot_ax=ax_0_typical)\n",
    "plot_patient_lines(15179, plot_ax=ax_0_non_typical)\n",
    "fig_0.suptitle(\"Class 0 timeline - typical and less typical \\n\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_1, (ax_1_typical, ax_1_non_typical) = plt.subplots(1,2)\n",
    "plot_patient_lines(9400, plot_ax=ax_1_typical)\n",
    "plot_patient_lines(13166, plot_ax=ax_1_non_typical)\n",
    "fig_1.suptitle(\"Class 1 timeline - typical and less typical \\n\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "fig_2, (ax_2_typical, ax_2_non_typical) = plt.subplots(1,2)\n",
    "fig_2.suptitle(\"Class 2 timeline - typical and less typical \\n\")\n",
    "plot_patient_lines(8345, plot_ax=ax_2_typical)\n",
    "plot_patient_lines(3520, plot_ax=ax_2_non_typical) \n",
    "plt.tight_layout()\n",
    "plt.rcParams['figure.figsize']  = curr_figsize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " curr_figsize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intermediate summary**\n",
    "\n",
    "- Each patient is characterized by his/her 3 timeseries.  \n",
    "- Visual inspection indicates that each class has a typical footprint in terms of its signals and their mutual configuration.\n",
    "- A reasonable direction would be to represent each signal as a 1st or 2nd order polynomial, and use the coefficient as the features set for each patient.\n",
    "- We can possibly add more features - e.g. whether or not signals intersect (typical to class 2), the average distance between signals (typically the y and z signals in class 1 are close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Possible additional schemes: whether or not the x and y signals interserct. \n",
    "extraction_schemes = ['parabola']\n",
    "ts_handling_scheme = ['use_ts', 'create_x']\n",
    "def extract_signals_params(signals_df, extraction_scheme, ts_handling_scheme):\n",
    "   \n",
    "    timestamps = signals_df.loc[:, 'timestamp']\n",
    "   \n",
    "    first_timestamp =  timestamps.iloc[0]\n",
    "    if ts_handling_scheme == 'use_ts':\n",
    "        x = np.array(list(map(lambda ts : (ts -first_timestamp).total_seconds()*1000, timestamps)))\n",
    "    else:\n",
    "        x = np.arange(0, len(timestamps)*50, 50) \n",
    "           \n",
    "    x = np.array([x, x*x]).T #When the extraction_schemt is 'parabole', actually.\n",
    "    sigs =  signals_df.loc[:, ['measurement_x', 'measurement_y', 'measurement_z']].values\n",
    "    #Regress each signal to a 2nd order polynomial\n",
    "    reg = LinearRegression().fit(x, sigs )\n",
    "    #Add the initial value of each signal, and flatten to a single list.\n",
    "    #Explicitly adding the intial value is redundant, since x starts at 0, hence the intercept is just this initial value\n",
    "    return list(np.concatenate((np.array([reg.intercept_]).T, reg.coef_), axis=1).reshape(1,-1)[0])\n",
    "    \n",
    "    \n",
    "def extract_single_patient_features( patient_df, extraction_scheme, ts_handling_scheme ):\n",
    "    patient_id = patient_df['patient_id'].iloc[0]\n",
    "    patient_features = []\n",
    "    curr_line_params = extract_signals_params(patient_df, extraction_scheme, ts_handling_scheme)\n",
    "    patient_features += curr_line_params\n",
    "    return patient_features\n",
    "        \n",
    "    \n",
    "def extract_features(in_df, extraction_scheme='parabola', ts_handling_scheme='use_ts'):\n",
    "    #TODO: input sanity checks\n",
    "    X = []\n",
    "    y = []\n",
    "    for _,patient_df in in_df.groupby('patient_id'):\n",
    "        extracted_features = extract_single_patient_features( patient_df, extraction_scheme, ts_handling_scheme )\n",
    "        X.append(extracted_features )\n",
    "        if 'label' in patient_df:\n",
    "            y.append(patient_df['label'].iloc[0])\n",
    "    features_names = ['meas_{}_a{}'.format(i,j) for i in ['x', 'y', 'z'] for j in range(3)]\n",
    "    return np.array(X), np.array(y), features_names\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, features_names  = extract_features(train_set_df, ts_handling_scheme='create_x')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check how well this model encodes the timeseries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patient_lines(8345, X[8345,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot $X|y$, just to get a feeling of how well the features are seperable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = X[ y==0, :]\n",
    "x_1 = X[ y==1, :]\n",
    "x_2 = X[ y==2, :]\n",
    "# len(x_0)\n",
    "fig_x_given_y, ax_x_given_y = plt.subplots()\n",
    "bins_f8 = np.arange(-0.0000001, 0.00000001, 0.000000001)\n",
    "bins_f0 = np.arange(0, 1.5, 0.01)\n",
    "bins_f2 = np.arange(-0.00000004, 0.00000004, 0.000000001)\n",
    "bins_f3 = np.arange(-0.2, 0.6, 0.01)\n",
    "ax_x_given_y.hist(x_0[:,0], bins=bins_f0);\n",
    "ax_x_given_y.hist(x_1[:,0], bins=bins_f0);\n",
    "ax_x_given_y.hist(x_2[:,0], bins=bins_f0);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d visualization (3 same order coefficients of the timeseries) indicate that classes 0, 1 are more seperable than class 2.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "i=2\n",
    "j=5\n",
    "k=8\n",
    "\n",
    "def plot_3d_features(i, j, k, title):\n",
    "    fig = go.Figure(data=[go.Scatter3d(x= x_0[:,i], y=x_0[:,j], z=x_0[:,k],\n",
    "                                       mode='markers', name='class 0', \n",
    "                                       marker=dict(\n",
    "                                        size=2,\n",
    "                                        opacity=0.6\n",
    "                                    ))])\n",
    "    fig.add_trace( go.Scatter3d(x= x_1[:,i], y=x_1[:,j], z=x_1[:,k],\n",
    "                                       mode='markers', name='class 1', \n",
    "                                       marker=dict(\n",
    "                                        size=2,\n",
    "                                        opacity=0.6\n",
    "                                    )))\n",
    "\n",
    "    fig.add_trace( go.Scatter3d(x= x_2[:,i], y=x_2[:,j], z=x_2[:,k],\n",
    "                                       mode='markers', name='class 2', \n",
    "                                       marker=dict(\n",
    "                                        size=2,\n",
    "                                        opacity=0.6\n",
    "                                    )))\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': title,\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'})\n",
    "    fig.show()\n",
    "\n",
    "plot_3d_features(0, 3, 6,  \"3 classes ts 0nd order coefficient\")\n",
    "plot_3d_features(2, 5, 8,  \"3 classes ts 2nd order coefficient\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, inds_train, inds_test = train_test_split(X, y, np.arange(len(y)), test_size = 0.2)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify the classes proportions remain.\n",
    "print(\"Classes proportions in train set: 0:{}, 1:{}, 2:{}\".format((y_train == 0).sum()/len(y_train), \n",
    "                                                                  (y_train == 1).sum()/len(y_train), \n",
    "                                                                  (y_train == 2).sum()/len(y_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find optimal meta parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierContainer:\n",
    "    \n",
    "    def __init__(self, clf, long_params_list, short_params_list):\n",
    "        self.clf = clf\n",
    "        self.long_params_list = long_params_list\n",
    "        self.short_params_list = short_params_list\n",
    "        \n",
    "    \n",
    "classifiers_dict = {\n",
    "    'svm': ClassifierContainer(SVC(),\n",
    "                    [{'kernel': ['rbf'], 'gamma': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30],\n",
    "                     'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 700, 800, 1000, 1200, 1500]},\n",
    "                    {'kernel': ['linear'], 'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]}\n",
    "                    ], \n",
    "                    [{'kernel': ['rbf'], 'gamma': [  1],\n",
    "                     'C': [300, 500, 700, 1000]}\n",
    "                    ]\n",
    "            ), \n",
    "                            \n",
    "    'random_forest' : ClassifierContainer(RandomForestClassifier(), \n",
    "                           [{ 'n_estimators' : [100, 200, 500, 1000, 1200, 1500],\n",
    "                              'max_depth': [ 10, 20, 30, 40, 50, 60, 100], \n",
    "                              'class_weight' :['balanced'] }\n",
    "                           ], \n",
    "                            [{ 'n_estimators' : [ 500, 600],\n",
    "                              'max_depth': [ 30, 40 ], \n",
    "                              'class_weight' :['balanced'] }\n",
    "                           ]\n",
    "                    ),\n",
    "    \n",
    "    'xgboost' : ClassifierContainer( xgb.XGBClassifier(), \n",
    "                           [{ 'reg_lambda' : [10, 30, 100], #[0.1, 0.3, 1, 3, 10, 30, 100, 300],\n",
    "                             'gamma' : [0.001, 0.003, 0.1, 0.3, 1, 3, 10, 30, 100, 300],\n",
    "#                                'reg_alpha':  [0.1, 0.3, 1, 3, 10, 30, 100, 300],\n",
    "                              'max_depth': [10,20,30], #[ 10, 20, 30, 40, 50, 60, 100], \n",
    "                              'n_jobs' : [5]\n",
    "                               }\n",
    "                           ], \n",
    "                             [{ 'reg_lambda' : [ 1],\n",
    "                             'gamma' : [ 100],\n",
    "                              'max_depth': [ 30], \n",
    "                               'n_jobs' : [5]\n",
    "                               }\n",
    "                           ]\n",
    "                    )\n",
    "            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_meta_parameters(train_x, train_y, classifier, tuned_parameters_dict, score='f1'):\n",
    "    \n",
    "    scores = [score]\n",
    "    #This one takes quite some time to complete...\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(\n",
    "           classifier, tuned_parameters_dict, scoring='%s_macro' % score, \n",
    "            verbose=10, \n",
    "            n_jobs=6\n",
    "        )\n",
    "        clf.fit(train_x, train_y)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "    return clf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_long_list = False\n",
    "tuned_classifiers = {}\n",
    "for clfname in classifiers_dict:\n",
    "    print( \"\\n ############## Tuning meta parameters for classifier {} ########### \\n\".format(clfname))\n",
    "    clf_params_to_tune = classifiers_dict[clfname]\n",
    "    curr_best_estimator = find_optimal_meta_parameters(X_train_scaled, y_train, clf_params_to_tune.clf, \n",
    "                                 clf_params_to_tune.long_params_list if tune_long_list else clf_params_to_tune.short_params_list,\n",
    "                                 score='f1')\n",
    "    tuned_classifiers[clfname] = curr_best_estimator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_importances(importances):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.title(\"Feature importances\")\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.bar(range(len(importances)), importances[indices],\n",
    "            color=\"r\",align=\"center\")\n",
    "    plt.xticks(range(len(importances)), np.array(features_names)[indices])\n",
    "    plt.xlim([-1, len(importances)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize the performance of each of the tuned classifiers, and find the one with max-min F1 score.\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "min_f1 = {}\n",
    "curr_figsize = plt.rcParams['figure.figsize']\n",
    "for clfname in tuned_classifiers:\n",
    "    print()\n",
    "    print( \"\\n################# cassification report for classifier {} ################\\n\".format(clfname))\n",
    "    clf = tuned_classifiers[clfname]\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print( classification_report(y_true, y_pred) )\n",
    "    print(\"\\n Confusion matrix (Wikipedia format):\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T) \n",
    "    print(\"\\n Confusion matrix (sklearn (transposed) format):\\n\")\n",
    "    plot_confusion_matrix(clf, X_test_scaled, y_true, display_labels=['class 0', 'class 1', 'class 2'], values_format='.0f')\n",
    "    plt.show()\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        plot_features_importances( clf.feature_importances_)\n",
    "    else:\n",
    "        print(\"Can't produce feature importance for classifier {}\".format(clfname))\n",
    "        \n",
    "    min_f1[clfname] = np.min(f1_score(y_true, y_pred, average=None))\n",
    "    print(\"\\n##################################################################################\\n\")\n",
    "    \n",
    "sorted_clfnames_by_scores = [k for k, v in sorted(min_f1.items(), key=lambda item: item[1])]\n",
    "best_classifier = tuned_classifiers[sorted_clfnames_by_scores[-1]]\n",
    "print(\"The classifier with the maximal minimal f1 score is {}\".format(sorted_clfnames_by_scores[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Use the entire training set to train the submission classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_for_submission,_,_  = extract_features(test_set_df, ts_handling_scheme='create_x')\n",
    "scaler_submission = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler_submission.transform(X)\n",
    "X_test_for_submission_scaled = scaler_submission.transform(X_test_for_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best params over the entire training set: \n",
    "svm: C:300, gamma: 0.03\n",
    "random forest: 'class_weight': 'balanced', 'max_depth': 60, 'n_estimators': 1200}\n",
    "xgboost: 0.944 (+/-0.015) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.943 (+/-0.013) for {'max_depth': 30, 'n_jobs': 5, 'reg_alpha': 0.3, 'reg_lambda': 30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.935 (+/-0.023) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.939 (+/-0.018) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.936 (+/-0.015) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.937 (+/-0.020) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.938 (+/-0.016) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.940 (+/-0.011) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.935 (+/-0.014) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.924 (+/-0.019) for {'max_depth': 10, 'n_jobs': 5, 'reg_lambda': 300}\n",
    "0.931 (+/-0.024) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.933 (+/-0.026) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.931 (+/-0.026) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.936 (+/-0.018) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.936 (+/-0.020) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.944 (+/-0.015) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.936 (+/-0.009) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.925 (+/-0.018) for {'max_depth': 20, 'n_jobs': 5, 'reg_lambda': 300}\n",
    "0.929 (+/-0.025) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.932 (+/-0.021) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.931 (+/-0.025) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.935 (+/-0.020) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.938 (+/-0.013) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.942 (+/-0.014) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.937 (+/-0.009) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.925 (+/-0.018) for {'max_depth': 30, 'n_jobs': 5, 'reg_lambda': 300}\n",
    "0.929 (+/-0.025) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.932 (+/-0.021) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.931 (+/-0.025) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.935 (+/-0.020) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.938 (+/-0.013) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.942 (+/-0.014) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.937 (+/-0.009) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.925 (+/-0.018) for {'max_depth': 40, 'n_jobs': 5, 'reg_lambda': 300}\n",
    "0.929 (+/-0.025) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.932 (+/-0.021) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.931 (+/-0.025) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.935 (+/-0.020) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.938 (+/-0.013) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.942 (+/-0.014) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.937 (+/-0.009) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.925 (+/-0.018) for {'max_depth': 50, 'n_jobs': 5, 'reg_lambda': 300}\n",
    "0.929 (+/-0.025) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.932 (+/-0.021) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.931 (+/-0.025) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.935 (+/-0.020) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.938 (+/-0.013) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.942 (+/-0.014) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.937 (+/-0.009) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.925 (+/-0.018) for {'max_depth': 60, 'n_jobs': 5, 'reg_lambda': 300}\n",
    "0.929 (+/-0.025) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 0.1}\n",
    "0.932 (+/-0.021) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 0.3}\n",
    "0.931 (+/-0.025) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 1}\n",
    "0.935 (+/-0.020) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 3}\n",
    "0.938 (+/-0.013) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 10}\n",
    "0.942 (+/-0.014) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 30}\n",
    "0.937 (+/-0.009) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 100}\n",
    "0.925 (+/-0.018) for {'max_depth': 100, 'n_jobs': 5, 'reg_lambda': 300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_long_list = True\n",
    "tuned_classifiers_submission = {}\n",
    "for clfname in ['xgboost']: #classifiers_dict:\n",
    "    print( \"\\n ############## Tuning meta parameters for classifier {} \\n\".format(clfname))\n",
    "    clf_params_to_tune = classifiers_dict[clfname]\n",
    "    curr_best_estimator = find_optimal_meta_parameters(X_scaled, y, clf_params_to_tune.clf, \n",
    "                                 clf_params_to_tune.long_params_list if tune_long_list else clf_params_to_tune.short_params_list,\n",
    "                                 score='f1')\n",
    "    tuned_classifiers[clfname] = curr_best_estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply on provided test set.\n",
    "y_test_hat = clf.predict(X_test_scaled)\n",
    "res_df = pd.DataFrame( { 'label' : y_test_hat, 'patient_id' : test_set_df.groupby('patient_id').nth(0).index.values})\n",
    "res_df.to_csv(\"clew_assignment_submission_matan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sklearn.metrics.f1_score(y_true, y_pred, average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30],\n",
    "                     'C': [1, 30, 100 ,300, 500, 800, 1000, 1500]}\n",
    "                   ]\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(\n",
    "       SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    \n",
    "# Now apply on provided test set.\n",
    "y_test_hat = clf.predict(X_test_scaled)\n",
    "res_df = pd.DataFrame( { 'label' : y_test_hat, 'patient_id' : test_set_df.groupby('patient_id').nth(0).index.values})\n",
    "res_df.to_csv(\"clew_assignment_submission_matan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM with gaussian kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SVM ####################\n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.debugger import set_trace\n",
    "# SVM with gaussian kernel\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30],\n",
    "                     'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 700, 800, 1000, 1500]},\n",
    "                    {'kernel': ['linear'], 'C': [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]}]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "#This one takes quite some time to complete...\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### SVM ####################\n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.debugger import set_trace\n",
    "# SVM with gaussian kernel\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [  1],\n",
    "                     'C': [300, 500, 800, 1000, 1500]}\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where have we missed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ i for i,val in enumerate((y_true == 2) & (y_pred == 1)) if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_test[1194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[240,:]\n",
    "plot_patient_lines(24602, X[24602,:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{  \n",
    "                       'n_estimators' : [100, 200, 500, 1000],\n",
    "                        'max_depth': [ 10, 20, 30, 40, 50, 60], \n",
    "                       'class_weight' :['balanced']\n",
    "                    }\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       RandomForestClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{ 'reg_lambda' : [0.1, 0.3, 1, 3, 10, 30, 100, 300],\n",
    "                             'reg_alpha ' : [0.1, 0.3, 1, 3, 10, 30, 100, 300],\n",
    "                              'max_depth': [ 10, 20, 30, 40, 50, 60], \n",
    "                               }\n",
    "                           ]\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       xgb.XGBClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc()\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Logistic Regression ########################\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# X_logit = scaler.transform(X_train)\n",
    "# X_logit_test = scaler.transform(X_test)\n",
    "clf = LogisticRegressionCV(cv=5, max_iter = 1000).fit(X_train_scaled, y_train)\n",
    "y_hat = clf.predict(X_test_scaled)\n",
    "# X_test_predicted_0 = X_test[y_hat==0]\n",
    "# X_test_predicted_1 = X_test[y_hat==1]\n",
    "# plt.scatter(X_test_predicted_0[:,0], X_test_predicted_0[:,1], c='y')\n",
    "# plt.scatter(X_test_predicted_1[:,0], X_test_predicted_1[:,1], c='b')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, y_hat, labels=[0, 1, 2])\n",
    "print(cnf_matrix.T)\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms \n",
    "clf_dict = {\"Logistic Regression\":LogisticRegression(penalty='l2', C=1, class_weight='balanced', solver='lbfgs'), # \n",
    "            \"Random Forest\": RandomForestClassifier(n_estimators=55, random_state=6),\n",
    "            \"XGBoost\": xgb.XGBClassifier()#(objective='multi:softmax')#, num_class=num_class\n",
    "           }\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "#     \"LogisiticRegression\": LogisticRegression(),\n",
    "#     \"KNearest\": KNeighborsClassifier(),\n",
    "#     \"Support Vector Classifier\": SVC(),\n",
    "#     \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "# }\n",
    "\n",
    "tuned_parameters = [{  'n_neighbors': [2, 3, 4]}\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       KNeighborsClassifier(), tuned_parameters, scoring='%s_macro' % score, cv=7\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = {\n",
    "#     \"LogisiticRegression\": LogisticRegression(),\n",
    "#     \"KNearest\": KNeighborsClassifier(),\n",
    "#     \"Support Vector Classifier\": SVC(),\n",
    "#     \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "# }\n",
    "\n",
    "tuned_parameters = [{  'Cs': [0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]], \n",
    "                        'class_weight' : [None, 'balanced']\n",
    "                    }\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       LogisticRegression(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{  'max_depth': [ 10, 20, 30, 40, 50, 60], \n",
    "                       'class_weight' :['balanced']\n",
    "                    }\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       DecisionTreeClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{  'max_depth': [ 4,5, 6, 10, 20, 30, 40, 50, 60], \n",
    "                       'class_weight' :['balanced']\n",
    "                    }\n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "       DecisionTreeClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "#     y_true, y_pred = y_test, clf.predict(X_test_scaled)\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print(confusion_matrix(y_true, y_pred, labels=[0,1,2]).T)\n",
    "#     print()\n",
    "\n",
    "# clf.fit(, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcv = pd.DataFrame(clf.cv_results_)\n",
    "dfcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcv[[\"split{}_test_score\".format(j) for j in range(7)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "# print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "# print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implementing SMOTE Technique \n",
    "# Cross Validating the right way\n",
    "# Parameters\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "for train, test in sss.split(X_train_scaled, y_train):\n",
    "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    model = pipeline.fit(X_train_scaled[train], y_train[train])\n",
    "    best_est = rand_log_reg.best_estimator_\n",
    "    prediction = best_est.predict(X_train_scaled[test])\n",
    "    \n",
    "    accuracy_lst.append(pipeline.score(X_train_scaled[test], y_train[test]))\n",
    "    precision_lst.append(precision_score(y_train[test], prediction))\n",
    "    recall_lst.append(recall_score(y_train[test], prediction))\n",
    "    f1_lst.append(f1_score(y_train[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(y_train[test], prediction))\n",
    "    \n",
    "print('---' * 45)\n",
    "print('')\n",
    "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
    "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
    "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
    "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
    "print('---' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.01, 0.03, 0.1],\n",
    "                     'C': [ 800, 1000]}\n",
    "                   ]\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(\n",
    "       SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(X_scaled, y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    \n",
    "# Now apply on provided test set.\n",
    "y_test_hat = clf.predict(X_test_scaled)\n",
    "res_df = pd.DataFrame( { 'label' : y_test_hat, 'patient_id' : test_set_df.groupby('patient_id').nth(0).index.values})\n",
    "res_df.to_csv(\"clew_assignment_submission_matan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test_of_submission_clf = clf.best_estimator_.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_hat_test_of_submission_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc_partial_train = SVC(C=1000, kernel='rbf', gamma=1)\n",
    "clf_svc_full_train = SVC(C=1000, kernel='rbf', gamma=0.03)\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "# X_test_raw_scaled = scaler.transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc_partial_train.fit(X_train_scaled, y_train)\n",
    "y_hat_partial = clf_svc_partial_train.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat_partial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc_full_train.fit(X_scaled, y)\n",
    "y_hat_full = clf_svc_full_train.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df.groupby('patient_id').nth(0).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame( { 'label' : y_test_hat, 'patient_id' : test_set_df.groupby('patient_id').nth(0).index.values})\n",
    "res_df.to_csv(\"clew_assignment_submission_matan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_binary = y_true\n",
    "y_true_binary[y_true_binary == 1] = 0\n",
    "y_true_binary[y_true_binary == 2] = 1\n",
    "np.unique(y_true_binary)\n",
    "print(np.unique(y_true))\n",
    "y_pred_binary = y_pred\n",
    "y_pred_binary[y_pred_binary == 1] = 0\n",
    "y_pred_binary[y_pred_binary == 2] = 1\n",
    "np.unique(y_pred_binary)\n",
    "all((y_pred_binary==1) == (y_pred == 2))\n",
    "# sklearn.metrics.f1_score(y_true_binary, y_pred_binary) \n",
    "print(classification_report(y_true_binary, y_pred_binary))\n",
    "print(confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1]).T)\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0, 1,2]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Remove outliers (corrupted ts)\n",
    "* Weigh signals based on their duration (a signal of 4 timestamps is less indicative than a 30 samples one). Additionally - apply classification on only the subset of signals with more than M timestamp and show scores over this set. \n",
    "* More features: do lines intersect? what's the average distane between two lines?\n",
    "* Beautify code: add parameters types, sanity tests and comments, remove debug printouts.\n",
    "\n",
    "* Add ROC\n",
    "* If possible - use these fancy auto analysis pandas tools.\n",
    "\n",
    "Theoretical:\n",
    "* squared v.s. abs regularization\n",
    "* Trees v.s. LR v.s. SVM - which one will be better under which circumstances? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting examples:\n",
    "* 15175, 15176 - two examples of 2 class which are quite different\n",
    "* 15178, 15179 - two examples of 0 class which seem identical, and also typical to type 2 class. More typical 0 ones: 1903\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_fab = [ 0,0,0,0,0,0,0,0, 1,1,1, 1, 2, 2, 2]\n",
    "y_hat_fab = [ 0,0,0,0,0,0,1,2, 1,1,0, 2, 1, 2, 2]\n",
    "print(classification_report(y_true_fab, y_hat_fab, labels=[0,1,2]))\n",
    "print(confusion_matrix(y_true_fab, y_hat_fab).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
