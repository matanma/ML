{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U -q ipywidgets\n",
    "# !pip3 install -U -q pyarrow\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Data Science Helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random \n",
    "\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)\n",
    "cf.set_config_file(colorscale='plotly', world_readable=True)\n",
    "\n",
    "# Extra options\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_columns = 25\n",
    "\n",
    "# Show all code cells outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import os\n",
    "from IPython.display import Image, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from sklearn.base import clone\n",
    " \n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some utility functions.\n",
    "\n",
    "class DataContainer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_x(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def set_y(self, y):\n",
    "        self.y = y\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def classification_assessment(X_test, y_test, y_test_predicted, clf, data_container=None):\n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    cnf_matrix = confusion_matrix(y_test, y_test_predicted, labels=[1,0])\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['incident=1','no incident=0'],normalize= False, \n",
    "                          title='Confusion matrix')\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    # plot ROC\n",
    "    if data_container is None:\n",
    "        plt.figure()\n",
    "    lr_probs = clf.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "#     print(lr_probs)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, ns_thresh = roc_curve(y_test, ns_probs)\n",
    "    lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    if data_container is None:\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "        # axis labels\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        # show the legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "    else: \n",
    "        roc_ax = data_container.roc_ax\n",
    "        roc_ax.plot(lr_fpr, lr_tpr, marker='.', label='Logistic, auc:{:0.2f}'.format(lr_auc))\n",
    "        roc_ax.legend()\n",
    "    return lr_probs\n",
    "    \n",
    "    \n",
    "def dilute_class(X, y, class_tag, dilute_factor):\n",
    "    y_indices = y==class_tag\n",
    "    num_class_entries = sum(y_indices)\n",
    "    diluted_ys_indices =  random.sample(range(num_class_entries), int(num_class_entries*dilute_factor))\n",
    "    diluted_ys = y[y_indices][diluted_ys_indices]\n",
    "    diluted_X_other_classes = X[~y_indices]\n",
    "    diluted_X_class = X[y_indices][diluted_ys_indices]\n",
    "    diluted_X = np.concatenate([diluted_X_other_classes, diluted_X_class])\n",
    "    diluted_y = np.concatenate([ y[~y_indices], diluted_ys])\n",
    "    \n",
    "    return diluted_X, diluted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708149fe0e5e4cd1975f62b670d03155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='num_samples', options=(500000, 10000, 1000000), value=500000), Dro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Choosing the initial dataset.\n",
    "#See https://plotly.com/python/histograms/\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "data_container = DataContainer()\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "\n",
    "@interact_manual\n",
    "def choose_dataset( num_samples=[500000, 10000, 1000000], \n",
    "                    ones_distribution=['normal', 'gev'],\n",
    "                    center_zero=widgets.FloatSlider(min=-2,max=-0.5,step=0.1,value=-1),\n",
    "                    center_one=widgets.FloatSlider(min=0.5,max=2,step=0.1,value=1),\n",
    "                    std_zero=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.8),\n",
    "                    std_one=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.5)\n",
    "                  ):\n",
    "    display(HTML(f'<h2>Plotting dataset of size {num_samples} <h2>'))\n",
    "    dataset_artificial_balanced_1_feature = \\\n",
    "                            make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one]],\n",
    "                                cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "    X, y = dataset_artificial_balanced_1_feature\n",
    "    if ones_distribution == 'gev':\n",
    "#         X_ones = np.random.exponential(10, sum(y))\n",
    "        X_ones = genextreme.rvs(c=1/300, loc=center_one, scale=std_one, size=sum(y))\n",
    "# u = np.random.rand(100000)\n",
    "# lmbd=1\n",
    "# k=1.5\n",
    "# weibull = lmbd*np.power((-np.log(1-u)),1/k) \n",
    "        \n",
    "        print(X_ones.shape)\n",
    "        print(X.shape)\n",
    "        X[y==1] = np.array([X_ones]).T\n",
    "        \n",
    "   \n",
    "    data_container.set_x(X)\n",
    "    data_container.set_y(y)\n",
    "    X_ones = X[y==1].T[0]\n",
    "    X_zeros = X[y==0].T[0]\n",
    "    print(\"1: mean: {:0.3f}, std: {:0.3f} \\n0: mean: {:0.3f}, std: {:0.3f} \".format(X_ones.mean(), X_ones.std(), X_zeros.mean(), X_zeros.std()))\n",
    " \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=X_zeros, nbinsx=500, histnorm='probability density', opacity=0.7, name='zeros'))\n",
    "    fig.add_trace(go.Histogram(x=X_ones, nbinsx=500, histnorm='probability density', opacity=0.7, name='ones'))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcca6bdb11304a029ed131abb5c557f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='zero_nsamples', options=(500000, 10000, 1000000), value=500000), D…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Split ones dataset\n",
    "\n",
    "@interact_manual\n",
    "def choose_dataset_split( zero_nsamples=[500000, 10000, 1000000], \n",
    "                          one_1_nsamples=[500000, 10000, 1000000], \n",
    "                          one_2_nsamples=[500000, 20000, 50000],\n",
    "                    center_zero=widgets.FloatSlider(min=-1,max=1,step=0.1,value=0),\n",
    "                    center_one_1=widgets.FloatSlider(min=1,max=3,step=0.1,value=2),\n",
    "                    center_one_2=widgets.FloatSlider(min=-4,max=-2,step=0.1,value=-3),\n",
    "                    std_zero=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.8),\n",
    "                    std_one_1=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.6),\n",
    "                    std_one_2=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.4)\n",
    "                  ):\n",
    "    display(HTML(f'<h2>Plotting dataset of size {num_samples} <h2>'))\n",
    "\n",
    "    dataset_split =  make_blobs(n_samples=[zero_nsamples, one_1_nsamples, one_2_nsamples ],\n",
    "                                n_features=1, centers=[[center_zero], [center_one_1],[center_one_2]],\n",
    "                                    cluster_std=[std_zero, std_one_1, std_one_2],  shuffle=False, random_state=4)\n",
    "\n",
    "#     dataset_artificial_balanced_1_feature = \\\n",
    "#                             make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one]],\n",
    "#                                 cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "    X, y = dataset_split\n",
    "    y[y==2] = 1\n",
    "    X = np.concatenate([X, X*X], axis=1)\n",
    "    data_container.set_x(X)\n",
    "    data_container.set_y(y)\n",
    "    X_ones = X[y==1].T[0]\n",
    "    X_zeros = X[y==0].T[0]\n",
    "    print(\"1: mean: {:0.3f}, std: {:0.3f} \\n0: mean: {:0.3f}, std: {:0.3f} \".format(X_ones.mean(), X_ones.std(), X_zeros.mean(), X_zeros.std()))\n",
    " \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=X_zeros, nbinsx=500, histnorm='probability density', opacity=0.7, name='zeros'))\n",
    "    fig.add_trace(go.Histogram(x=X_ones, nbinsx=500, histnorm='probability density', opacity=0.7, name='ones'))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"Classifier's probabilities over test set\")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'prob(y(x)=1)')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ROC curves')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fpr')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'tpr')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c6fe7d10>]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1bd13c090>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c6fe7190>]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c6fe7050>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the external figure, and plot the empirical probability \n",
    "# %matplotlib qt\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_container.X, data_container.y, test_size=0.2)\n",
    "data_container.X_train = X_train\n",
    "data_container.X_test = X_test\n",
    "data_container.y_train = y_train\n",
    "data_container.y_test = y_test\n",
    "\n",
    "probs_fig, probs_axs = plt.subplots(2,1)\n",
    "data_container.probs_ax, data_container.roc_ax = probs_axs\n",
    "data_container.probs_ax.set_title(\"Classifier's probabilities over test set\")\n",
    "data_container.probs_ax.set_xlabel(\"x\")\n",
    "data_container.probs_ax.set_ylabel(\"prob(y(x)=1)\")\n",
    "\n",
    "data_container.roc_ax.set_title(\"ROC curves\")\n",
    "data_container.roc_ax.set_xlabel(\"fpr\")\n",
    "data_container.roc_ax.set_ylabel(\"tpr\")\n",
    "\n",
    "def onpick(event):\n",
    "    print(\"in onpick\")\n",
    "    # on the pick event, find the orig line corresponding to the\n",
    "    # legend proxy line, and toggle the visibility\n",
    "    # print(\"Just entered onpick!!\")\n",
    "    legline = event.artist\n",
    "    origlines = lined[legline]\n",
    "    vis = not origlines[0].get_visible()\n",
    "    for origline in origlines:\n",
    "        origline.set_visible(vis)\n",
    "    # Change the alpha on the line in the legend so we can see what lines\n",
    "    # have been toggled\n",
    "    if vis:\n",
    "        legline.set_alpha(1.0)\n",
    "    else:\n",
    "        legline.set_alpha(0.2)\n",
    "    probs_fig.canvas.draw()\n",
    "\n",
    "probs_fig.canvas.mpl_connect('pick_event', onpick)    \n",
    "data_container.probs_fig = probs_fig\n",
    "\n",
    "#Overlay the empirical probabilities (taken from the actual \"real\" distributions).\n",
    "def calc_bin_prob(bn_df):\n",
    "#     set_trace()\n",
    "    bin_x = bn_df[0].left\n",
    "    df = bn_df[1]\n",
    "    prob_1 = np.nan if len(df) == 0 else df['y'].sum()/len(df) \n",
    "    return bin_x, prob_1\n",
    "\n",
    "X = data_container.X\n",
    "y = data_container.y\n",
    "X_y_df = pd.DataFrame({'X':X.T[0], 'y':y})\n",
    "#Extend range otherwise inerval may be nan.\n",
    "X_y_df['x_binned'] = pd.cut(X_y_df['X'], bins=np.arange(X.min()-0.05, X.max() + 0.05, 0.05))\n",
    "x_binned = []\n",
    "prob_one_binned = []\n",
    "for bn_df in X_y_df.groupby('x_binned'):\n",
    "    bin_x, prob_1 = calc_bin_prob(bn_df)\n",
    "    x_binned.append(bin_x)\n",
    "    prob_one_binned.append(prob_1)\n",
    "prob_one_binned = np.array(prob_one_binned)\n",
    "data_container.probs_ax.plot(x_binned, prob_one_binned, 'r', label=\"empirical P(y=1)\")\n",
    "# data_container.probs_ax.plot(x_binned, prob_one_binned/(1-prob_one_binned), 'r+', label=\"empirical P(y=1)/P(y=0)\")\n",
    "# data_container.probs_ax.plot(x_binned, np.log(prob_one_binned/(1-prob_one_binned)), 'rx', label=\"empirical log(P(y=1)/P(y=0))\")\n",
    "\n",
    "data_container.probs_ax.legend()\n",
    "#Roc\n",
    "x_test_binned = X_y_df.set_index('X').loc[X_test.T[0],'x_binned'].values\n",
    "# if np.nan in x_test_binned:\n",
    "#     print(\"Found NAN!!\")\n",
    "#     for i,intr in enumerate(x_test_binned):\n",
    "#         if isinstance(intr, float):\n",
    "#             print(f\"The nan is at index {i}, corresponding to x_test {X_test[i]}\")\n",
    "def get_left(interval):\n",
    "    return interval.left\n",
    "\n",
    "\n",
    "x_test_binned = list(map(get_left , x_test_binned))\n",
    "empirical_probs = pd.DataFrame({'probs':prob_one_binned}, index = x_binned).loc[x_test_binned]\n",
    "fpr, tpr, thresh = roc_curve(y_test, empirical_probs)\n",
    "emp_auc = roc_auc_score(y_test, empirical_probs)\n",
    "data_container.roc_ax.plot(fpr, tpr, 'r', label=\"empirical, auc:{:0.2f}\".format(emp_auc))\n",
    "data_container.roc_ax.legend()\n",
    "#TODO: calc recall, precision, F1 over the empirical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now, let's apply a standard classification and assess the results.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     49860\n",
      "           1       0.92      0.92      0.92     50140\n",
      "\n",
      "    accuracy                           0.92    100000\n",
      "   macro avg       0.92      0.92      0.92    100000\n",
      "weighted avg       0.92      0.92      0.92    100000\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[46304  3836]\n",
      " [ 3770 46090]]\n",
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bdaa7d10>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1aa840ed0>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "X_train = data_container.X_train\n",
    "y_train = data_container.y_train\n",
    "X_test = data_container.X_test\n",
    "y_test = data_container.y_test\n",
    "\n",
    "\n",
    "clf_lr = LogisticRegressionCV(cv=5, random_state=4).fit(X_train, y_train)\n",
    "y_hat_lr = clf_lr.predict(X_test)\n",
    "prediction_probs = classification_assessment(X_test, y_test, y_hat_lr, clf_lr, data_container)\n",
    "# %matplotlib qt\n",
    "data_container.probs_ax.scatter(X_test[:,0], prediction_probs, s=2, label=f\"Baseline logistic\")\n",
    "\n",
    "\n",
    "data_container.probs_ax.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Next, we're going to dilute the '1' class to obtain an imbalanced dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99007e22cfe4254bb663b2b018eca1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='dilute_factor', max=1.0, min=0.01, step=0.01), Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@interact_manual\n",
    "def dilute_ones( dilute_factor=widgets.FloatSlider(min=0.01,max=1,step=0.01,value=0.01)):\n",
    "    X = data_container.X\n",
    "    y = data_container.y\n",
    "    #Dilute the 1's class\n",
    "    diluted_X, diluted_y = dilute_class(X, y, 1, dilute_factor)\n",
    "    data_container.one_dilute_factor = dilute_factor\n",
    "    data_container.diluted_X = diluted_X\n",
    "    data_container.diluted_y = diluted_y\n",
    "    #Dilute the train set separately, since we want to evaluate over the original test set, \n",
    "    # so we don't want any of it to be used for training. \n",
    "    diluted_X_train, diluted_y_train = dilute_class(data_container.X_train, data_container.y_train, 1, dilute_factor)\n",
    "    data_container.diluted_X_train_ones = diluted_X_train\n",
    "    data_container.diluted_y_train_ones = diluted_y_train\n",
    "    \n",
    "    \n",
    "    diluted_X_zeros = diluted_X[diluted_y == 0]\n",
    "    diluted_X_ones = diluted_X[diluted_y == 1]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=diluted_X_zeros.T[0], nbinsx=1000, name=\"zeros\"))\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_ones.T[0], nbinsx=1000, name=\"ones\"))\n",
    "    fig.show()\n",
    "    \n",
    "    hist_ax = data_container.probs_ax.twinx()\n",
    "    hist_ax.hist(diluted_X_zeros.T[0], bins=1000, alpha=0.1, color='b')\n",
    "    hist_ax.hist(diluted_X_ones.T[0], bins=1000, alpha=0.1, color='r')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diluted_clf = LogisticRegressionCV(cv=5, random_state=0).fit(data_container.diluted_X, data_container.diluted_y)\n",
    "# print(\"Before correction: Theta0_D: {}, theta1_D: {}\".format(diluted_clf.intercept_, diluted_clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Now, let's examine several methods to cope with the imbalance. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345f99a7bead4dae9886a91256b02329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='zero_dilute_factor', max=1.0, min=0.01, step=0.01), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel/eventloops.py:106: UserWarning:\n",
      "\n",
      "Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel/eventloops.py:106: UserWarning:\n",
      "\n",
      "Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel/eventloops.py:106: UserWarning:\n",
      "\n",
      "Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "#Predict on diluted data, optionally dilute 0 class as well.\n",
    "def generate_corrected_clf(X, y, correction_method, tau, y_bar):\n",
    "    \n",
    "    if correction_method == 'none':\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "    #Apply correction\n",
    "    if correction_method == \"prior\":\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "        corrected_intercept = corrected_clf.intercept_ - np.log( (1-tau)/tau*y_bar/(1-y_bar)    )\n",
    "        corrected_clf.intercept_ = corrected_intercept\n",
    "       \n",
    "    if correction_method == \"weighting\":\n",
    "        w1 = tau/y_bar\n",
    "        w0 = (1-tau)/(1-y_bar)\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0, class_weight={0:w0, 1:w1}).fit(X, y)\n",
    "    if correction_method == 'gev':\n",
    "        corrected_clf = GevRegressionCV().fit(X,y)\n",
    "    return corrected_clf\n",
    "        \n",
    "@interact_manual\n",
    "def dilute_zeros_and_predict( zero_dilute_factor=widgets.FloatSlider(min=0.01,max=1,step=0.01,value=1), \n",
    "                           correction_method = ['none', 'prior', 'weighting', 'gev']):\n",
    "    diluted_X = data_container.diluted_X\n",
    "    diluted_y = data_container.diluted_y\n",
    "    #Dilute the 0's class\n",
    "   \n",
    "    diluted_1_0_X, diluted_1_0_y = dilute_class(diluted_X, diluted_y, 0, zero_dilute_factor)\n",
    "    data_container.diluted_X_ones_and_zeros = diluted_1_0_X\n",
    "    data_container.diluted_y_ones_and_zeros = diluted_1_0_y\n",
    "    #Dilute the train set separately, since we want to evaluate over the original test set, \n",
    "    # so we don't want any of it to be used for training. \n",
    "    diluted_X_train_ones_and_zeros, diluted_y_train_ones_and_zeros =\\\n",
    "        dilute_class(data_container.diluted_X_train_ones, data_container.diluted_y_train_ones, 0, zero_dilute_factor)\n",
    "    data_container.diluted_X_train_ones_and_zeros = diluted_X_train_ones_and_zeros\n",
    "    data_container.diluted_y_train_ones_and_zeros = diluted_y_train_ones_and_zeros\n",
    "    \n",
    "    \n",
    "    \n",
    "    diluted_X_zeros = diluted_1_0_X[diluted_1_0_y == 0]\n",
    "    diluted_X_ones = diluted_1_0_X[diluted_1_0_y == 1]\n",
    "    y = data_container.y\n",
    "    tau = sum(y)/len(y) #The actual population ratio, before any dilution.\n",
    "    y_bar =  sum(diluted_1_0_y)/len(diluted_1_0_y) #The sample ratio.\n",
    "    #Predict\n",
    "    display(HTML(f'<h3>Using a dataset of size {len(diluted_1_0_y)}, {len(diluted_X_zeros)} of which are 0,\\\n",
    "    and {len(diluted_X_ones)} are 1 <h3>'))\n",
    "    print(\"Before performing classification over the diluted data.\")\n",
    "    print(\"Original dataset: zeros: {}, ones: {}\".format(sum(data_container.y==0), sum(data_container.y == 1)))\n",
    "    print(\"Diluted training dataset: zeros: {}, ones: {}\".format(sum(data_container.diluted_y_train_ones_and_zeros==0), \n",
    "                                                        sum(data_container.diluted_y_train_ones_and_zeros == 1)))\n",
    "    \n",
    "    corrected_clf = generate_corrected_clf(data_container.diluted_X_train_ones_and_zeros, \n",
    "                                           data_container.diluted_y_train_ones_and_zeros, \n",
    "                                           correction_method, tau, y_bar)\n",
    "    X_test = data_container.X_test\n",
    "    y_test = data_container.y_test\n",
    "    y_hat = corrected_clf.predict(X_test)\n",
    "    prediction_probs = classification_assessment(X_test, data_container.y_test, \n",
    "                                                 y_hat, corrected_clf, data_container)\n",
    "#     print(prediction_probs)\n",
    "#     print(prediction_probs.shape)\n",
    "#     print(sum(prediction_probs > 0.5))\n",
    "#     probs = corrected_clf.predict_proba(X_test)\n",
    "    data_container.probs = prediction_probs\n",
    "#     print(probs)\n",
    "#     print(\"######################{}###\".format(any(probs[:,1] != prediction_probs )))\n",
    "    data_container.probs_ax.scatter(X_test[:,0], prediction_probs, s=2, \n",
    "                                    label=\"0:{}%_1:{}%_corr:{}\".format(\n",
    "                                    int(zero_dilute_factor*100), int(data_container.one_dilute_factor*100), \n",
    "                                        correction_method))\n",
    "    data_container.probs_ax.legend()\n",
    "#     lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, probs[:,1])\n",
    "#     print(sum(probs[:,1]>0.5))\n",
    "#     print(lr_tpr)\n",
    "#     data_container.roc_ax.scatter(lr_fpr, lr_tpr)\n",
    "# #     probs_fig = go.Figure()\n",
    "#     probs_fig.add_trace(go.Scatter( x= X_test.T, y=probs[:,1].T ))\n",
    "#     probs_fig.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    fig = go.Figure();\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_zeros.T[0], nbinsx=1000))\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_ones.T[0], nbinsx=1000))\n",
    "    fig.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.63, -0.61,  0.13, ...,  1.51,  1.1 ,  0.46])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "X_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)\n",
    "probs = np.array(0.5*np.random.rand(len(y_test)))\n",
    "sum(probs>0.5)\n",
    "lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, probs)\n",
    "print(lr_thresh)\n",
    "plt.plot(lr_fpr, lr_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diluted_X = data_container.diluted_X\n",
    "diluted_y = data_container.diluted_y\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0).fit(diluted_X, diluted_y)\n",
    "y_hat_tmp = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "print(sum(probs>0.5))\n",
    "fpr, tpr, thresh = roc_curve(y_test, y_hat_tmp)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GEV #######################\n",
    "# import scipy.optimize.fmin_cg\n",
    "def gev(x, tau):\n",
    "    return np.exp( -np.power(1 + tau*x, -1/tau))\n",
    "\n",
    "\n",
    "class GevRegressionCV():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.curr_iter_x_dot_theta = None\n",
    "        self.curr_iter_pi = None\n",
    "    \n",
    "    @staticmethod \n",
    "    def err_func(theta_tau, X, y, reg_param, gev_instance):\n",
    "        x_dot_theta = np.dot(X, theta)\n",
    "        pi = gev(x_dot_theta)\n",
    "        h_theta = -1/len(y) *( np.dot(y, np.log(pi)) + np.dot(1-y, np.log(1 - pi))) + \n",
    "            reg_param/(2*len(y))*np.dot(theta_tau, theta_tau)\n",
    "        #Save for the gradient invocation, to save repeated evaluation. \n",
    "#         gev_instance.curr_iter_x_dot_theta =  x_dot_theta\n",
    "#         gev_instance.curr_iter_pi = pi\n",
    "        print(f\"h_theta: {h_theta}\")\n",
    "        return h_theta\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad(theta_tau, X, y, reg_param, gev_instance):\n",
    "        #These two are calculated twice. Make this more efficient later if necessary. \n",
    "        x_dot_theta = np.dot(X, theta)\n",
    "        pi = gev(x_dot_theta)\n",
    "#         x_dot_theta = gev_instance.curr_iter_x_dot_theta\n",
    "#         pi = gev_instance.curr_iter_pi\n",
    "        mult_vec = np.log(pi)*(y-pi)/( (1+tau*x_dot_theta)*(1-pi) )\n",
    "        grad_theta = -np.dot(X.T, mult_vec)\n",
    "        #Regularization\n",
    "        grad_theta[1:] += reg_param/len(y)*theta_tau[1:]\n",
    "        \n",
    "        u = 1/(tau*tau)*np.log((1+tau*x_dot_theta)) - x_dot_theta/( tau*(1+tau*x_dot_theta))\n",
    "        v = (y-pi)*np.log(pi)/(1-pi)\n",
    "        grad_tau = -np.dot(u,v) #TODO: regularize tau as well?\n",
    "        \n",
    "        return h_theta, np.append([grad_theta, grad_tau])\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        #This function requires two separate functions calls - one for the log-likelihood, \n",
    "        # and the other for the gradient calculation. I'm saving the \n",
    "        #Add intercept\n",
    "        first_col = np.array([np.ones(X.shape[0])])\n",
    "        X = np.concatenate((b.T, X), axis=1)\n",
    "        initial_guess\n",
    "        opt_theta_tau = scipy.optimize.fmin_cg(self.err_func, initial_guess,\n",
    "                                               fprime=self.grad, args=(X, y, self) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([6.83e-06, 0.00e+00, 6.83e-06, 6.83e-06, 1.37e-05, 6.83e-06,\n",
       "        6.83e-06, 6.83e-06, 6.83e-06, 6.83e-06, 6.83e-06, 0.00e+00,\n",
       "        6.83e-06, 1.37e-05, 6.83e-06, 2.05e-05, 2.73e-05, 1.37e-05,\n",
       "        2.73e-05, 2.73e-05, 2.73e-05, 2.73e-05, 2.73e-05, 4.78e-05,\n",
       "        6.83e-05, 3.41e-05, 8.87e-05, 8.87e-05, 1.02e-04, 7.51e-05,\n",
       "        9.56e-05, 8.19e-05, 1.02e-04, 1.02e-04, 1.64e-04, 1.30e-04,\n",
       "        8.87e-05, 1.91e-04, 1.02e-04, 1.91e-04, 1.37e-04, 1.71e-04,\n",
       "        1.84e-04, 2.05e-04, 1.84e-04, 2.18e-04, 1.98e-04, 2.18e-04,\n",
       "        2.93e-04, 2.46e-04, 2.93e-04, 3.62e-04, 3.69e-04, 3.55e-04,\n",
       "        3.48e-04, 5.05e-04, 4.23e-04, 4.50e-04, 6.42e-04, 5.53e-04,\n",
       "        5.80e-04, 5.39e-04, 5.80e-04, 6.07e-04, 5.46e-04, 7.64e-04,\n",
       "        6.89e-04, 9.08e-04, 7.99e-04, 6.55e-04, 7.99e-04, 1.05e-03,\n",
       "        9.76e-04, 8.87e-04, 9.56e-04, 9.49e-04, 9.97e-04, 1.09e-03,\n",
       "        1.24e-03, 1.24e-03, 1.06e-03, 1.30e-03, 1.19e-03, 1.43e-03,\n",
       "        1.44e-03, 1.46e-03, 1.47e-03, 1.58e-03, 1.29e-03, 1.55e-03,\n",
       "        1.54e-03, 1.73e-03, 1.72e-03, 1.75e-03, 1.82e-03, 1.90e-03,\n",
       "        1.85e-03, 1.99e-03, 2.14e-03, 2.05e-03, 2.09e-03, 2.27e-03,\n",
       "        2.41e-03, 2.37e-03, 2.46e-03, 2.62e-03, 2.60e-03, 2.39e-03,\n",
       "        2.42e-03, 2.47e-03, 2.44e-03, 2.74e-03, 2.47e-03, 2.81e-03,\n",
       "        2.72e-03, 2.67e-03, 2.81e-03, 3.04e-03, 3.11e-03, 2.68e-03,\n",
       "        2.79e-03, 2.96e-03, 3.13e-03, 2.79e-03, 3.02e-03, 3.13e-03,\n",
       "        3.15e-03, 3.25e-03, 3.32e-03, 3.31e-03, 3.04e-03, 3.20e-03,\n",
       "        3.52e-03, 3.34e-03, 3.37e-03, 3.30e-03, 3.37e-03, 3.27e-03,\n",
       "        3.46e-03, 3.60e-03, 3.58e-03, 3.69e-03, 3.73e-03, 3.36e-03,\n",
       "        3.74e-03, 3.54e-03, 3.89e-03, 3.82e-03, 3.74e-03, 3.85e-03,\n",
       "        3.66e-03, 3.62e-03, 3.49e-03, 3.63e-03, 3.67e-03, 3.72e-03,\n",
       "        3.61e-03, 3.61e-03, 3.77e-03, 3.56e-03, 3.41e-03, 3.85e-03,\n",
       "        3.58e-03, 3.73e-03, 3.69e-03, 3.64e-03, 3.70e-03, 3.76e-03,\n",
       "        3.70e-03, 3.56e-03, 3.34e-03, 3.67e-03, 3.72e-03, 3.43e-03,\n",
       "        3.76e-03, 3.70e-03, 3.30e-03, 3.50e-03, 3.50e-03, 3.48e-03,\n",
       "        3.50e-03, 3.46e-03, 3.36e-03, 3.52e-03, 3.47e-03, 3.29e-03,\n",
       "        3.46e-03, 3.45e-03, 3.15e-03, 3.56e-03, 3.61e-03, 3.04e-03,\n",
       "        3.25e-03, 3.13e-03, 3.31e-03, 3.52e-03, 3.17e-03, 3.08e-03,\n",
       "        3.30e-03, 3.40e-03, 2.97e-03, 3.31e-03, 3.43e-03, 3.19e-03,\n",
       "        3.11e-03, 3.03e-03, 3.11e-03, 3.08e-03, 3.06e-03, 3.05e-03,\n",
       "        3.06e-03, 3.00e-03, 3.13e-03, 3.03e-03, 2.93e-03, 2.76e-03,\n",
       "        2.80e-03, 2.87e-03, 2.63e-03, 2.64e-03, 2.56e-03, 2.69e-03,\n",
       "        2.75e-03, 2.58e-03, 2.63e-03, 2.50e-03, 2.81e-03, 2.55e-03,\n",
       "        2.52e-03, 2.68e-03, 2.61e-03, 2.62e-03, 2.64e-03, 2.50e-03,\n",
       "        2.41e-03, 2.42e-03, 2.25e-03, 2.55e-03, 2.31e-03, 2.45e-03,\n",
       "        2.46e-03, 2.18e-03, 2.16e-03, 2.16e-03, 2.27e-03, 2.20e-03,\n",
       "        2.14e-03, 2.35e-03, 2.17e-03, 2.10e-03, 1.99e-03, 2.16e-03,\n",
       "        2.14e-03, 1.88e-03, 2.09e-03, 1.78e-03, 1.78e-03, 1.75e-03,\n",
       "        1.95e-03, 1.95e-03, 1.69e-03, 1.73e-03, 1.92e-03, 2.05e-03,\n",
       "        1.72e-03, 1.93e-03, 1.62e-03, 1.82e-03, 1.74e-03, 1.71e-03,\n",
       "        1.67e-03, 1.58e-03, 1.55e-03, 1.76e-03, 1.65e-03, 1.58e-03,\n",
       "        1.69e-03, 1.48e-03, 1.41e-03, 1.55e-03, 1.45e-03, 1.40e-03,\n",
       "        1.45e-03, 1.34e-03, 1.41e-03, 1.24e-03, 1.43e-03, 1.28e-03,\n",
       "        1.38e-03, 1.17e-03, 1.40e-03, 1.12e-03, 1.34e-03, 1.28e-03,\n",
       "        1.43e-03, 1.24e-03, 1.01e-03, 1.21e-03, 1.09e-03, 1.17e-03,\n",
       "        1.17e-03, 1.13e-03, 1.14e-03, 1.13e-03, 1.02e-03, 1.06e-03,\n",
       "        9.83e-04, 1.07e-03, 8.87e-04, 1.12e-03, 1.04e-03, 1.02e-03,\n",
       "        1.04e-03, 8.05e-04, 8.05e-04, 1.11e-03, 8.87e-04, 9.01e-04,\n",
       "        8.80e-04, 8.19e-04, 9.56e-04, 9.21e-04, 8.33e-04, 8.26e-04,\n",
       "        7.92e-04, 7.71e-04, 7.37e-04, 8.12e-04, 7.17e-04, 8.12e-04,\n",
       "        6.83e-04, 8.05e-04, 7.10e-04, 6.76e-04, 6.55e-04, 7.64e-04,\n",
       "        6.14e-04, 6.55e-04, 6.83e-04, 6.48e-04, 6.35e-04, 6.83e-04,\n",
       "        6.96e-04, 6.14e-04, 6.21e-04, 5.32e-04, 6.35e-04, 6.55e-04,\n",
       "        7.23e-04, 5.12e-04, 5.87e-04, 5.32e-04, 6.35e-04, 5.67e-04,\n",
       "        6.07e-04, 5.12e-04, 5.19e-04, 6.48e-04, 5.19e-04, 4.98e-04,\n",
       "        5.19e-04, 4.37e-04, 5.46e-04, 5.19e-04, 4.91e-04, 4.91e-04,\n",
       "        4.85e-04, 5.19e-04, 4.64e-04, 4.91e-04, 4.91e-04, 5.39e-04,\n",
       "        4.16e-04, 4.37e-04, 4.16e-04, 4.50e-04, 4.85e-04, 3.62e-04,\n",
       "        4.37e-04, 3.14e-04, 4.03e-04, 4.16e-04, 3.69e-04, 3.96e-04,\n",
       "        3.69e-04, 3.55e-04, 3.96e-04, 3.75e-04, 3.14e-04, 2.80e-04,\n",
       "        3.62e-04, 3.69e-04, 3.48e-04, 3.48e-04, 3.28e-04, 3.07e-04,\n",
       "        3.96e-04, 3.00e-04, 2.73e-04, 2.87e-04, 2.25e-04, 3.07e-04,\n",
       "        2.73e-04, 3.00e-04, 3.21e-04, 3.28e-04, 2.59e-04, 2.53e-04,\n",
       "        2.80e-04, 2.05e-04, 2.59e-04, 2.80e-04, 2.05e-04, 2.59e-04,\n",
       "        2.53e-04, 2.53e-04, 1.84e-04, 3.00e-04, 1.91e-04, 2.39e-04,\n",
       "        2.32e-04, 2.39e-04, 1.77e-04, 2.53e-04, 3.34e-04, 1.50e-04,\n",
       "        1.50e-04, 1.84e-04, 2.18e-04, 1.84e-04, 1.37e-04, 1.71e-04,\n",
       "        2.05e-04, 1.98e-04, 2.05e-04, 2.25e-04, 1.30e-04, 1.77e-04,\n",
       "        1.77e-04, 1.91e-04, 1.64e-04, 1.16e-04, 1.50e-04, 2.46e-04,\n",
       "        1.71e-04, 1.43e-04, 1.84e-04, 2.05e-04, 1.50e-04, 1.50e-04,\n",
       "        1.98e-04, 1.50e-04, 1.02e-04, 1.71e-04, 1.50e-04, 1.57e-04,\n",
       "        8.87e-05, 1.57e-04, 1.64e-04, 1.37e-04, 1.02e-04, 1.43e-04,\n",
       "        1.23e-04, 1.57e-04, 6.83e-05, 1.37e-04, 1.16e-04, 8.87e-05,\n",
       "        1.43e-04, 1.09e-04, 5.46e-05, 1.71e-04, 1.23e-04, 8.87e-05,\n",
       "        6.14e-05, 1.16e-04, 1.09e-04, 1.43e-04, 1.02e-04, 1.30e-04,\n",
       "        6.83e-05, 1.37e-04, 1.02e-04, 8.19e-05, 9.56e-05, 3.41e-05,\n",
       "        6.83e-05, 8.87e-05, 9.56e-05, 4.78e-05, 9.56e-05, 1.16e-04,\n",
       "        6.83e-05, 8.87e-05, 7.51e-05, 8.19e-05, 1.02e-04, 6.83e-05,\n",
       "        8.87e-05, 1.02e-04, 8.87e-05, 6.83e-05, 4.78e-05, 5.46e-05,\n",
       "        7.51e-05, 3.41e-05, 3.41e-05, 5.46e-05, 6.14e-05, 4.10e-05,\n",
       "        4.78e-05, 4.78e-05, 2.05e-05, 2.73e-05, 8.19e-05, 8.19e-05,\n",
       "        9.56e-05, 4.78e-05, 6.14e-05, 6.14e-05, 2.73e-05, 6.14e-05,\n",
       "        3.41e-05, 2.05e-05, 5.46e-05, 7.51e-05, 6.14e-05, 4.78e-05,\n",
       "        4.78e-05, 4.78e-05, 3.41e-05, 2.73e-05, 4.10e-05, 3.41e-05,\n",
       "        4.10e-05, 2.05e-05, 4.10e-05, 4.10e-05, 5.46e-05, 3.41e-05,\n",
       "        4.10e-05, 4.78e-05, 4.10e-05, 6.14e-05, 2.05e-05, 2.73e-05,\n",
       "        4.78e-05, 7.51e-05, 2.73e-05, 4.10e-05, 3.41e-05, 2.73e-05,\n",
       "        3.41e-05, 4.78e-05, 2.05e-05, 3.41e-05, 2.73e-05, 1.37e-05,\n",
       "        2.73e-05, 2.73e-05, 1.37e-05, 1.37e-05, 3.41e-05, 1.37e-05,\n",
       "        2.05e-05, 3.41e-05, 2.05e-05, 2.05e-05, 4.10e-05, 1.37e-05,\n",
       "        3.41e-05, 2.73e-05, 1.37e-05, 2.05e-05, 2.73e-05, 2.73e-05,\n",
       "        6.83e-06, 6.83e-06, 4.78e-05, 1.37e-05, 2.05e-05, 4.10e-05,\n",
       "        3.41e-05, 2.05e-05, 2.05e-05, 6.83e-06, 1.37e-05, 1.37e-05,\n",
       "        2.05e-05, 6.83e-06, 6.83e-06, 1.37e-05, 1.37e-05, 6.83e-06,\n",
       "        1.37e-05, 2.05e-05, 2.05e-05, 2.73e-05, 3.41e-05, 1.37e-05,\n",
       "        2.05e-05, 2.73e-05, 1.37e-05, 6.83e-06, 1.37e-05, 1.37e-05,\n",
       "        1.37e-05, 6.83e-06, 3.41e-05, 2.05e-05, 1.37e-05, 0.00e+00,\n",
       "        2.73e-05, 1.37e-05, 0.00e+00, 2.73e-05, 2.05e-05, 1.37e-05,\n",
       "        0.00e+00, 6.83e-06, 2.73e-05, 0.00e+00, 6.83e-06, 6.83e-06,\n",
       "        6.83e-06, 2.05e-05, 1.37e-05, 1.37e-05, 6.83e-06, 1.37e-05,\n",
       "        2.73e-05, 0.00e+00, 1.37e-05, 6.83e-06, 0.00e+00, 2.05e-05,\n",
       "        6.83e-06, 1.37e-05, 0.00e+00, 1.37e-05, 6.83e-06, 6.83e-06,\n",
       "        6.83e-06, 2.05e-05, 0.00e+00, 6.83e-06, 1.37e-05, 1.37e-05,\n",
       "        0.00e+00, 1.37e-05, 0.00e+00, 6.83e-06, 6.83e-06, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 1.37e-05, 0.00e+00, 0.00e+00, 6.83e-06,\n",
       "        6.83e-06, 2.05e-05, 6.83e-06, 0.00e+00, 0.00e+00, 6.83e-06,\n",
       "        0.00e+00, 6.83e-06, 6.83e-06, 1.37e-05, 6.83e-06, 0.00e+00,\n",
       "        1.37e-05, 6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06,\n",
       "        6.83e-06, 0.00e+00, 0.00e+00, 1.37e-05, 6.83e-06, 0.00e+00,\n",
       "        6.83e-06, 6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00, 2.05e-05,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 1.37e-05, 6.83e-06, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06, 1.37e-05,\n",
       "        0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00, 6.83e-06, 6.83e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        6.83e-06, 1.37e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06, 6.83e-06,\n",
       "        6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        6.83e-06, 0.00e+00, 1.37e-05, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        1.37e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 6.83e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 1.37e-05, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00, 1.37e-05,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 6.83e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 6.83e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00,\n",
       "        0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        6.83e-06, 6.83e-06, 0.00e+00, 6.83e-06, 6.83e-06, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00, 6.83e-06, 6.83e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 6.83e-06, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 6.83e-06]),\n",
       " array([-236.66, -235.19, -233.73, ..., 1225.52, 1226.98, 1228.45]),\n",
       " <a list of 1000 Patch objects>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import genextreme\n",
    "r = genextreme.rvs(c=-0.0000000007, loc=0, scale=100, size=100000)\n",
    "plt.figure()\n",
    "plt.hist(r, bins = 1000, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.08, 0.13, 0.18, 0.21, 0.22, 0.25, 0.27, 0.3 , 0.28, 0.32, 0.32,\n",
       "        0.37, 0.31, 0.42, 0.4 , 0.36, 0.38, 0.46, 0.38, 0.41, 0.45, 0.39,\n",
       "        0.49, 0.45, 0.48, 0.47, 0.45, 0.48, 0.48, 0.5 , 0.51, 0.5 , 0.49,\n",
       "        0.58, 0.54, 0.55, 0.58, 0.61, 0.57, 0.55, 0.56, 0.6 , 0.69, 0.62,\n",
       "        0.61, 0.64, 0.63, 0.66, 0.61, 0.62, 0.6 , 0.63, 0.65, 0.65, 0.63,\n",
       "        0.63, 0.66, 0.68, 0.65, 0.69, 0.68, 0.67, 0.74, 0.69, 0.7 , 0.7 ,\n",
       "        0.69, 0.76, 0.74, 0.7 , 0.73, 0.74, 0.73, 0.66, 0.74, 0.71, 0.69,\n",
       "        0.76, 0.74, 0.78, 0.75, 0.74, 0.79, 0.69, 0.78, 0.71, 0.8 , 0.72,\n",
       "        0.72, 0.76, 0.81, 0.71, 0.85, 0.7 , 0.74, 0.73, 0.74, 0.72, 0.75,\n",
       "        0.78, 0.73, 0.79, 0.8 , 0.81, 0.72, 0.76, 0.75, 0.77, 0.76, 0.73,\n",
       "        0.75, 0.74, 0.68, 0.75, 0.73, 0.73, 0.72, 0.68, 0.77, 0.74, 0.77,\n",
       "        0.74, 0.74, 0.73, 0.78, 0.77, 0.72, 0.71, 0.75, 0.73, 0.69, 0.77,\n",
       "        0.77, 0.7 , 0.73, 0.76, 0.74, 0.76, 0.7 , 0.73, 0.7 , 0.75, 0.73,\n",
       "        0.7 , 0.71, 0.74, 0.69, 0.71, 0.73, 0.66, 0.67, 0.76, 0.72, 0.67,\n",
       "        0.61, 0.7 , 0.65, 0.66, 0.65, 0.68, 0.66, 0.73, 0.68, 0.76, 0.64,\n",
       "        0.69, 0.68, 0.7 , 0.71, 0.69, 0.71, 0.64, 0.67, 0.67, 0.75, 0.59,\n",
       "        0.68, 0.61, 0.72, 0.69, 0.61, 0.68, 0.63, 0.59, 0.68, 0.59, 0.64,\n",
       "        0.65, 0.65, 0.6 , 0.62, 0.58, 0.53, 0.62, 0.59, 0.59, 0.57, 0.63,\n",
       "        0.61, 0.59, 0.57, 0.57, 0.61, 0.59, 0.58, 0.54, 0.57, 0.59, 0.56,\n",
       "        0.55, 0.52, 0.48, 0.6 , 0.51, 0.57, 0.62, 0.57, 0.55, 0.55, 0.59,\n",
       "        0.57, 0.57, 0.54, 0.59, 0.57, 0.51, 0.53, 0.51, 0.53, 0.54, 0.56,\n",
       "        0.51, 0.46, 0.51, 0.58, 0.46, 0.47, 0.52, 0.5 , 0.56, 0.53, 0.55,\n",
       "        0.51, 0.5 , 0.56, 0.5 , 0.43, 0.54, 0.46, 0.48, 0.53, 0.42, 0.48,\n",
       "        0.49, 0.45, 0.47, 0.46, 0.42, 0.44, 0.48, 0.41, 0.47, 0.42, 0.45,\n",
       "        0.43, 0.44, 0.42, 0.46, 0.38, 0.39, 0.45, 0.4 , 0.46, 0.42, 0.43,\n",
       "        0.41, 0.36, 0.42, 0.42, 0.41, 0.38, 0.37, 0.41, 0.44, 0.37, 0.39,\n",
       "        0.4 , 0.35, 0.35, 0.37, 0.36, 0.42, 0.36, 0.36, 0.38, 0.32, 0.35,\n",
       "        0.37, 0.37, 0.3 , 0.31, 0.33, 0.33, 0.31, 0.36, 0.35, 0.33, 0.37,\n",
       "        0.3 , 0.32, 0.35, 0.31, 0.34, 0.31, 0.32, 0.35, 0.31, 0.26, 0.31,\n",
       "        0.28, 0.34, 0.34, 0.26, 0.29, 0.28, 0.29, 0.28, 0.34, 0.28, 0.28,\n",
       "        0.31, 0.28, 0.26, 0.34, 0.28, 0.33, 0.29, 0.27, 0.27, 0.23, 0.27,\n",
       "        0.28, 0.29, 0.24, 0.22, 0.26, 0.26, 0.25, 0.25, 0.22, 0.24, 0.23,\n",
       "        0.26, 0.26, 0.22, 0.25, 0.19, 0.24, 0.26, 0.2 , 0.2 , 0.22, 0.23,\n",
       "        0.24, 0.24, 0.25, 0.22, 0.23, 0.21, 0.18, 0.22, 0.18, 0.19, 0.18,\n",
       "        0.25, 0.24, 0.21, 0.19, 0.18, 0.23, 0.2 , 0.19, 0.2 , 0.15, 0.17,\n",
       "        0.19, 0.19, 0.2 , 0.18, 0.17, 0.15, 0.13, 0.16, 0.17, 0.12, 0.2 ,\n",
       "        0.18, 0.15, 0.16, 0.15, 0.18, 0.18, 0.17, 0.17, 0.14, 0.16, 0.15,\n",
       "        0.15, 0.15, 0.13, 0.14, 0.13, 0.17, 0.17, 0.17, 0.12, 0.18, 0.13,\n",
       "        0.18, 0.16, 0.17, 0.17, 0.15, 0.14, 0.13, 0.14, 0.17, 0.08, 0.11,\n",
       "        0.14, 0.15, 0.11, 0.15, 0.14, 0.16, 0.09, 0.13, 0.13, 0.1 , 0.1 ,\n",
       "        0.13, 0.15, 0.12, 0.15, 0.15, 0.11, 0.14, 0.12, 0.14, 0.1 , 0.12,\n",
       "        0.1 , 0.13, 0.09, 0.1 , 0.13, 0.1 , 0.1 , 0.15, 0.1 , 0.11, 0.07,\n",
       "        0.08, 0.1 , 0.08, 0.1 , 0.09, 0.08, 0.1 , 0.09, 0.1 , 0.09, 0.1 ,\n",
       "        0.11, 0.1 , 0.08, 0.12, 0.09, 0.06, 0.09, 0.08, 0.11, 0.08, 0.11,\n",
       "        0.09, 0.08, 0.07, 0.11, 0.09, 0.08, 0.08, 0.08, 0.08, 0.06, 0.06,\n",
       "        0.07, 0.08, 0.06, 0.07, 0.08, 0.07, 0.09, 0.08, 0.08, 0.08, 0.05,\n",
       "        0.09, 0.04, 0.05, 0.06, 0.07, 0.08, 0.07, 0.09, 0.07, 0.07, 0.07,\n",
       "        0.05, 0.07, 0.08, 0.09, 0.04, 0.04, 0.05, 0.07, 0.05, 0.04, 0.06,\n",
       "        0.06, 0.05, 0.04, 0.05, 0.04, 0.06, 0.05, 0.04, 0.05, 0.05, 0.04,\n",
       "        0.06, 0.05, 0.06, 0.03, 0.03, 0.03, 0.04, 0.05, 0.05, 0.03, 0.05,\n",
       "        0.05, 0.04, 0.02, 0.05, 0.05, 0.03, 0.05, 0.05, 0.06, 0.05, 0.06,\n",
       "        0.03, 0.04, 0.06, 0.03, 0.03, 0.04, 0.06, 0.02, 0.03, 0.03, 0.03,\n",
       "        0.02, 0.02, 0.03, 0.04, 0.04, 0.03, 0.02, 0.03, 0.04, 0.03, 0.03,\n",
       "        0.03, 0.02, 0.02, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.03, 0.04,\n",
       "        0.02, 0.02, 0.02, 0.03, 0.02, 0.02, 0.03, 0.02, 0.04, 0.03, 0.03,\n",
       "        0.02, 0.02, 0.03, 0.04, 0.01, 0.01, 0.03, 0.02, 0.01, 0.01, 0.03,\n",
       "        0.03, 0.02, 0.03, 0.03, 0.02, 0.02, 0.02, 0.03, 0.03, 0.01, 0.02,\n",
       "        0.01, 0.01, 0.02, 0.01, 0.01, 0.03, 0.02, 0.03, 0.01, 0.02, 0.02,\n",
       "        0.01, 0.01, 0.02, 0.01, 0.02, 0.03, 0.01, 0.  , 0.01, 0.01, 0.02,\n",
       "        0.01, 0.02, 0.02, 0.01, 0.02, 0.01, 0.01, 0.02, 0.01, 0.01, 0.01,\n",
       "        0.01, 0.01, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  ,\n",
       "        0.02, 0.01, 0.02, 0.01, 0.02, 0.02, 0.  , 0.01, 0.01, 0.01, 0.01,\n",
       "        0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.01, 0.02,\n",
       "        0.01, 0.01, 0.02, 0.01, 0.01, 0.  , 0.01, 0.01, 0.01, 0.01, 0.  ,\n",
       "        0.01, 0.  , 0.01, 0.01, 0.  , 0.  , 0.01, 0.  , 0.01, 0.01, 0.01,\n",
       "        0.  , 0.01, 0.  , 0.  , 0.01, 0.  , 0.  , 0.01, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.  , 0.01, 0.  , 0.  ,\n",
       "        0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.01, 0.  , 0.01, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01,\n",
       "        0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.  ,\n",
       "        0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ,\n",
       "        0.  , 0.01, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]),\n",
       " array([2.46e-04, 4.82e-03, 9.40e-03, ..., 4.57e+00, 4.57e+00, 4.58e+00]),\n",
       " <a list of 1000 Patch objects>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.random.rand(100000)\n",
    "lmbd=1\n",
    "k=1.5\n",
    "weibull = lmbd*np.power((-np.log(1-u)),1/k) \n",
    "plt.hist(weibull, bins = 1000, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.01e-02, 9.92e-03, 9.48e-03, 9.89e-03, 9.75e-03, 9.30e-03,\n",
       "        9.28e-03, 9.58e-03, 8.97e-03, 8.43e-03, 8.80e-03, 8.85e-03,\n",
       "        8.54e-03, 8.32e-03, 8.10e-03, 8.30e-03, 7.87e-03, 7.88e-03,\n",
       "        7.79e-03, 7.45e-03, 7.36e-03, 7.33e-03, 7.59e-03, 7.49e-03,\n",
       "        7.28e-03, 6.81e-03, 7.35e-03, 7.04e-03, 6.68e-03, 6.64e-03,\n",
       "        6.68e-03, 6.32e-03, 6.79e-03, 6.53e-03, 6.61e-03, 6.68e-03,\n",
       "        6.32e-03, 5.79e-03, 5.94e-03, 5.74e-03, 5.59e-03, 6.13e-03,\n",
       "        5.69e-03, 6.17e-03, 5.59e-03, 5.72e-03, 5.49e-03, 5.28e-03,\n",
       "        5.65e-03, 5.03e-03, 4.92e-03, 5.21e-03, 4.92e-03, 4.77e-03,\n",
       "        4.71e-03, 4.76e-03, 5.16e-03, 4.60e-03, 4.82e-03, 4.72e-03,\n",
       "        4.39e-03, 4.66e-03, 4.59e-03, 4.79e-03, 4.40e-03, 4.05e-03,\n",
       "        4.47e-03, 4.22e-03, 4.42e-03, 4.29e-03, 3.84e-03, 4.06e-03,\n",
       "        3.92e-03, 3.57e-03, 4.30e-03, 3.91e-03, 3.81e-03, 3.84e-03,\n",
       "        3.79e-03, 3.63e-03, 3.31e-03, 3.41e-03, 3.36e-03, 3.03e-03,\n",
       "        3.26e-03, 3.43e-03, 3.09e-03, 2.99e-03, 3.26e-03, 3.04e-03,\n",
       "        3.06e-03, 3.05e-03, 2.87e-03, 3.08e-03, 2.86e-03, 2.70e-03,\n",
       "        2.92e-03, 2.66e-03, 2.94e-03, 2.56e-03, 2.39e-03, 2.81e-03,\n",
       "        2.92e-03, 2.61e-03, 2.69e-03, 2.80e-03, 2.61e-03, 2.38e-03,\n",
       "        2.70e-03, 2.58e-03, 2.66e-03, 2.57e-03, 2.33e-03, 2.50e-03,\n",
       "        2.45e-03, 2.27e-03, 2.10e-03, 1.87e-03, 2.14e-03, 2.05e-03,\n",
       "        2.13e-03, 2.17e-03, 1.98e-03, 2.02e-03, 2.12e-03, 1.87e-03,\n",
       "        1.83e-03, 1.88e-03, 1.73e-03, 2.04e-03, 1.69e-03, 1.66e-03,\n",
       "        1.92e-03, 1.75e-03, 1.97e-03, 1.86e-03, 1.58e-03, 1.97e-03,\n",
       "        1.42e-03, 1.69e-03, 1.53e-03, 1.54e-03, 1.59e-03, 1.61e-03,\n",
       "        1.55e-03, 1.71e-03, 1.52e-03, 1.35e-03, 1.36e-03, 1.45e-03,\n",
       "        1.22e-03, 1.45e-03, 1.30e-03, 1.40e-03, 1.45e-03, 1.37e-03,\n",
       "        1.19e-03, 1.33e-03, 1.27e-03, 1.27e-03, 1.16e-03, 1.31e-03,\n",
       "        1.09e-03, 1.25e-03, 1.35e-03, 1.14e-03, 9.28e-04, 1.16e-03,\n",
       "        1.14e-03, 1.22e-03, 1.08e-03, 1.21e-03, 1.04e-03, 1.11e-03,\n",
       "        9.82e-04, 8.66e-04, 1.01e-03, 9.59e-04, 1.06e-03, 1.17e-03,\n",
       "        8.74e-04, 1.16e-03, 9.05e-04, 9.90e-04, 1.22e-03, 9.36e-04,\n",
       "        9.28e-04, 1.04e-03, 7.12e-04, 9.05e-04, 8.59e-04, 8.20e-04,\n",
       "        7.97e-04, 7.58e-04, 8.74e-04, 6.73e-04, 9.13e-04, 7.66e-04,\n",
       "        6.88e-04, 7.19e-04, 7.19e-04, 7.89e-04, 7.12e-04, 6.88e-04,\n",
       "        6.03e-04, 6.34e-04, 6.27e-04, 5.41e-04, 5.96e-04, 7.27e-04,\n",
       "        6.03e-04, 6.50e-04, 6.34e-04, 5.49e-04, 5.03e-04, 5.96e-04,\n",
       "        4.95e-04, 7.19e-04, 6.34e-04, 5.88e-04, 6.34e-04, 6.03e-04,\n",
       "        4.10e-04, 6.19e-04, 6.57e-04, 4.95e-04, 5.34e-04, 4.80e-04,\n",
       "        6.11e-04, 5.80e-04, 5.03e-04, 5.18e-04, 4.80e-04, 4.41e-04,\n",
       "        4.41e-04, 5.11e-04, 3.56e-04, 4.25e-04, 4.95e-04, 3.79e-04,\n",
       "        5.41e-04, 4.56e-04, 4.02e-04, 4.33e-04, 3.48e-04, 4.41e-04,\n",
       "        4.18e-04, 4.80e-04, 4.80e-04, 3.79e-04, 3.48e-04, 4.02e-04,\n",
       "        4.02e-04, 4.64e-04, 3.64e-04, 3.25e-04, 3.09e-04, 3.17e-04,\n",
       "        3.94e-04, 3.40e-04, 2.71e-04, 3.56e-04, 3.40e-04, 4.02e-04,\n",
       "        2.94e-04, 4.10e-04, 2.94e-04, 3.79e-04, 3.25e-04, 2.71e-04,\n",
       "        3.17e-04, 2.40e-04, 2.63e-04, 2.48e-04, 2.63e-04, 2.40e-04,\n",
       "        3.09e-04, 2.01e-04, 2.71e-04, 3.33e-04, 3.02e-04, 2.40e-04,\n",
       "        2.32e-04, 2.24e-04, 3.25e-04, 2.86e-04, 3.25e-04, 2.32e-04,\n",
       "        2.86e-04, 3.09e-04, 2.32e-04, 1.93e-04, 2.55e-04, 2.40e-04,\n",
       "        1.31e-04, 1.93e-04, 2.86e-04, 2.94e-04, 2.40e-04, 2.09e-04,\n",
       "        2.48e-04, 1.86e-04, 2.24e-04, 1.86e-04, 2.63e-04, 1.70e-04,\n",
       "        1.62e-04, 1.93e-04, 1.55e-04, 1.31e-04, 1.39e-04, 2.01e-04,\n",
       "        1.55e-04, 1.93e-04, 2.17e-04, 2.24e-04, 2.24e-04, 1.78e-04,\n",
       "        1.24e-04, 1.70e-04, 2.17e-04, 1.78e-04, 1.31e-04, 2.17e-04,\n",
       "        9.28e-05, 1.78e-04, 1.47e-04, 1.78e-04, 2.24e-04, 2.01e-04,\n",
       "        1.31e-04, 1.47e-04, 1.62e-04, 1.70e-04, 1.01e-04, 1.01e-04,\n",
       "        1.47e-04, 1.47e-04, 7.74e-05, 1.31e-04, 1.39e-04, 1.08e-04,\n",
       "        1.62e-04, 1.31e-04, 1.24e-04, 8.51e-05, 6.19e-05, 1.16e-04,\n",
       "        1.16e-04, 8.51e-05, 2.01e-04, 9.28e-05, 1.78e-04, 4.64e-05,\n",
       "        1.08e-04, 1.08e-04, 9.28e-05, 8.51e-05, 8.51e-05, 1.01e-04,\n",
       "        4.64e-05, 1.16e-04, 1.55e-04, 1.01e-04, 7.74e-05, 1.08e-04,\n",
       "        1.08e-04, 7.74e-05, 1.39e-04, 1.24e-04, 1.01e-04, 1.01e-04,\n",
       "        7.74e-05, 8.51e-05, 5.41e-05, 8.51e-05, 6.96e-05, 5.41e-05,\n",
       "        4.64e-05, 7.74e-05, 6.96e-05, 6.96e-05, 7.74e-05, 5.41e-05,\n",
       "        3.09e-05, 7.74e-05, 5.41e-05, 1.55e-05, 5.41e-05, 6.96e-05,\n",
       "        6.19e-05, 8.51e-05, 9.28e-05, 6.96e-05, 3.87e-05, 3.09e-05,\n",
       "        6.19e-05, 3.09e-05, 3.87e-05, 9.28e-05, 4.64e-05, 1.16e-04,\n",
       "        1.01e-04, 3.87e-05, 3.87e-05, 6.96e-05, 5.41e-05, 4.64e-05,\n",
       "        3.87e-05, 3.87e-05, 4.64e-05, 3.09e-05, 6.96e-05, 7.74e-05,\n",
       "        3.87e-05, 5.41e-05, 3.87e-05, 2.32e-05, 4.64e-05, 6.19e-05,\n",
       "        1.55e-05, 3.09e-05, 5.41e-05, 3.09e-05, 6.19e-05, 6.19e-05,\n",
       "        1.55e-05, 8.51e-05, 4.64e-05, 1.55e-05, 7.74e-06, 3.87e-05,\n",
       "        4.64e-05, 4.64e-05, 3.09e-05, 3.09e-05, 4.64e-05, 2.32e-05,\n",
       "        3.87e-05, 3.09e-05, 2.32e-05, 2.32e-05, 3.87e-05, 4.64e-05,\n",
       "        3.87e-05, 2.32e-05, 3.09e-05, 1.55e-05, 1.55e-05, 3.09e-05,\n",
       "        1.55e-05, 0.00e+00, 4.64e-05, 3.87e-05, 3.87e-05, 3.87e-05,\n",
       "        4.64e-05, 1.55e-05, 7.74e-06, 2.32e-05, 5.41e-05, 2.32e-05,\n",
       "        2.32e-05, 7.74e-06, 3.09e-05, 1.55e-05, 3.09e-05, 2.32e-05,\n",
       "        1.55e-05, 1.55e-05, 2.32e-05, 3.87e-05, 7.74e-06, 3.09e-05,\n",
       "        1.55e-05, 3.87e-05, 1.55e-05, 1.55e-05, 1.55e-05, 7.74e-06,\n",
       "        7.74e-06, 1.55e-05, 7.74e-06, 2.32e-05, 1.55e-05, 1.55e-05,\n",
       "        3.09e-05, 2.32e-05, 7.74e-06, 7.74e-06, 2.32e-05, 2.32e-05,\n",
       "        7.74e-06, 1.55e-05, 3.87e-05, 7.74e-06, 1.55e-05, 1.55e-05,\n",
       "        0.00e+00, 4.64e-05, 1.55e-05, 3.09e-05, 7.74e-06, 1.55e-05,\n",
       "        1.55e-05, 1.55e-05, 2.32e-05, 1.55e-05, 0.00e+00, 3.09e-05,\n",
       "        3.87e-05, 1.55e-05, 7.74e-06, 1.55e-05, 0.00e+00, 1.55e-05,\n",
       "        3.09e-05, 1.55e-05, 0.00e+00, 1.55e-05, 3.09e-05, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 1.55e-05, 7.74e-06, 1.55e-05, 7.74e-06,\n",
       "        1.55e-05, 0.00e+00, 7.74e-06, 7.74e-06, 1.55e-05, 7.74e-06,\n",
       "        0.00e+00, 7.74e-06, 7.74e-06, 0.00e+00, 0.00e+00, 1.55e-05,\n",
       "        0.00e+00, 7.74e-06, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 1.55e-05, 7.74e-06, 7.74e-06, 7.74e-06, 7.74e-06,\n",
       "        1.55e-05, 7.74e-06, 7.74e-06, 0.00e+00, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 1.55e-05, 0.00e+00, 1.55e-05, 1.55e-05, 1.55e-05,\n",
       "        0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06, 0.00e+00, 0.00e+00,\n",
       "        1.55e-05, 7.74e-06, 0.00e+00, 1.55e-05, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 1.55e-05,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06, 0.00e+00,\n",
       "        7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00,\n",
       "        1.55e-05, 7.74e-06, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00,\n",
       "        0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 2.32e-05, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        7.74e-06, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 7.74e-06, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06,\n",
       "        7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        1.55e-05, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00,\n",
       "        0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 7.74e-06, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00, 0.00e+00,\n",
       "        0.00e+00, 0.00e+00, 0.00e+00, 7.74e-06]),\n",
       " array([3.96e-04, 1.29e+00, 2.59e+00, ..., 1.29e+03, 1.29e+03, 1.29e+03]),\n",
       " <a list of 1000 Patch objects>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expn = np.random.exponential(100, 100000)\n",
    "plt.hist(expn, bins=1000, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blobs test\n",
    "num_samples = 200000\n",
    "center_zero = 0\n",
    "center_one_1 = 2\n",
    "center_one_2 = -3\n",
    "std_zero = 0.7\n",
    "std_one = 0.6\n",
    "    \n",
    "dataset1 =  make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one_1], \n",
    "                                                                                     [center_one_2]],\n",
    "                                cluster_std=[std_zero, std_one, std_one],  shuffle=False, random_state=4) \n",
    "     \n",
    "# dataset1 =  make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one_1]],\n",
    "#                             cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "X, y = dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y==2]=1\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = X[y==1]\n",
    "X_zeros = X[y==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots()\n",
    "bins = np.arange(-5,4,0.05);\n",
    "ax.hist(X_ones, bins=bins, label='ones', alpha=.3, density=True)\n",
    "ax.hist(X_zeros, bins=bins, label='zeros', alpha=0.3, density=True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020000, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1020000, 4)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1020000,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "XX = np.concatenate([X, X*X], axis=1)\n",
    "XX.shape\n",
    "X[0]*X[0] == XX[0,1]\n",
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(40000, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13db20490>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     13384\n",
      "           1       0.97      0.96      0.96     26616\n",
      "\n",
      "    accuracy                           0.95     40000\n",
      "   macro avg       0.94      0.95      0.95     40000\n",
      "weighted avg       0.95      0.95      0.95     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(XX, y, test_size=0.2)\n",
    "clf = LogisticRegressionCV(cv=5, random_state=4).fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "probs.shape\n",
    "X_test.shape\n",
    "ax.scatter(X_test[:,0], probs, color='r')\n",
    "\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "diluted_X, diluted_y = dilute_class(X, y, 1, 0.1)\n",
    "diluted_X, diluted_y = dilute_class(diluted_X, diluted_y, 0, 0.1)\n",
    "plt.scatter(diluted_X[:,0], diluted_y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([clf.intercept_, clf.coef_])\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = np.concatenate([np.ones(len(X_test)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(np.arange(-2,2,0.05))\n",
    "y = np.log(1+np.exp(2*x))\n",
    "plt.plot(x,y, x , np.log(np.exp(2*x           )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14aca3390>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fpr, lr_tpr, lr_thresh = roc_curve(data_container.y_test, data_container.probs)\n",
    "plt.figure()\n",
    "# plt.scatter(data_container.X_test, data_container.probs)\n",
    "plt.plot(lr_fpr, lr_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1455b0910>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13ab84f90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(data_container.X_test, data_container.y_test)\n",
    "plt.scatter(data_container.X_test, data_container.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6405"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr_tpr)\n",
    "#plt.scatter(data_container.X_test, lr_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ad33c90>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(lr_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1359b7050>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "precision, recall, thresholds = precision_recall_curve( data_container.y_test, data_container.probs)\n",
    "plt.plot(  recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58427"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thresholds)\n",
    "len(np.unique(data_container.probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, ..., 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
