{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U -q ipywidgets\n",
    "# !pip3 install -U -q pyarrow\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Data Science Helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random \n",
    "\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)\n",
    "cf.set_config_file(colorscale='plotly', world_readable=True)\n",
    "\n",
    "# Extra options\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_columns = 25\n",
    "\n",
    "# Show all code cells outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import os\n",
    "from IPython.display import Image, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from sklearn.base import clone\n",
    " \n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some utility functions.\n",
    "\n",
    "class DataContainer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_x(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def set_y(self, y):\n",
    "        self.y = y\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def classification_assessment(X_test, y_test, y_test_predicted, clf, data_container=None):\n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    cnf_matrix = confusion_matrix(y_test, y_test_predicted, labels=[1,0])\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['incident=1','no incident=0'],normalize= False, \n",
    "                          title='Confusion matrix')\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    # plot ROC\n",
    "    if data_container is None:\n",
    "        plt.figure()\n",
    "    lr_probs = clf.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "#     print(lr_probs)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, ns_thresh = roc_curve(y_test, ns_probs)\n",
    "    lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    if data_container is None:\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "        # axis labels\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        # show the legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "    else: \n",
    "        roc_ax = data_container.roc_ax\n",
    "        roc_ax.plot(lr_fpr, lr_tpr, marker='.', label='Logistic, auc:{:0.2f}'.format(lr_auc))\n",
    "        roc_ax.legend()\n",
    "    return lr_probs\n",
    "    \n",
    "    \n",
    "def dilute_class(X, y, class_tag, dilute_factor):\n",
    "    y_indices = y==class_tag\n",
    "    num_class_entries = sum(y_indices)\n",
    "    diluted_ys_indices =  random.sample(range(num_class_entries), int(num_class_entries*dilute_factor))\n",
    "    diluted_ys = y[y_indices][diluted_ys_indices]\n",
    "    diluted_X_other_classes = X[~y_indices]\n",
    "    diluted_X_class = X[y_indices][diluted_ys_indices]\n",
    "    diluted_X = np.concatenate([diluted_X_other_classes, diluted_X_class])\n",
    "    diluted_y = np.concatenate([ y[~y_indices], diluted_ys])\n",
    "    \n",
    "    return diluted_X, diluted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ac591a72d4483aa52650b378b88f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='num_samples', options=(500000, 10000, 1000000), value=500000), Floâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Choosing the initial dataset.\n",
    "#See https://plotly.com/python/histograms/\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "data_container = DataContainer()\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "\n",
    "@interact_manual\n",
    "def choose_dataset( num_samples=[500000, 10000, 1000000], \n",
    "                    center_zero=widgets.FloatSlider(min=-2,max=-0.5,step=0.1,value=-1),\n",
    "                    center_one=widgets.FloatSlider(min=0.5,max=2,step=0.1,value=1),\n",
    "                    std_zero=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.8),\n",
    "                    std_one=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.5)\n",
    "                  ):\n",
    "    display(HTML(f'<h2>Plotting dataset of size {num_samples} <h2>'))\n",
    "    dataset_artificial_balanced_1_feature = \\\n",
    "                            make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one]],\n",
    "                                cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "    X, y = dataset_artificial_balanced_1_feature\n",
    "    data_container.set_x(X)\n",
    "    data_container.set_y(y)\n",
    "    X_ones = X[y==1].T[0]\n",
    "    X_zeros = X[y==0].T[0]\n",
    "    print(\"1: mean: {:0.3f}, std: {:0.3f} \\n0: mean: {:0.3f}, std: {:0.3f} \".format(X_ones.mean(), X_ones.std(), X_zeros.mean(), X_zeros.std()))\n",
    " \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=X_zeros, nbinsx=500, histnorm='probability density', opacity=0.7, name='zeros'))\n",
    "    fig.add_trace(go.Histogram(x=X_ones, nbinsx=500, histnorm='probability density', opacity=0.7, name='ones'))\n",
    "    fig.show()\n",
    "\n",
    "#     fig, ax_hist = plt.subplots()\n",
    "#     ax_hist.hist(X_ones, bins = 500, density=True, alpha = 0.7, label=\"ones\")\n",
    "#     ax_hist.hist(X_zeros, bins = 500, density=True, alpha = 0.7, label = \"zeros\")\n",
    "#     ax_hist.legend()\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"Classifier's probabilities over test set\")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'prob(y(x)=1)')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ROC curves')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fpr')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'tpr')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x143fabed0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x143fb2350>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1437e9110>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12a631790>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the external figure, and plot the empirical probability \n",
    "# %matplotlib qt\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_container.X, data_container.y, test_size=0.2)\n",
    "data_container.X_train = X_train\n",
    "data_container.X_test = X_test\n",
    "data_container.y_train = y_train\n",
    "data_container.y_test = y_test\n",
    "\n",
    "probs_fig, probs_axs = plt.subplots(2,1)\n",
    "data_container.probs_ax, data_container.roc_ax = probs_axs\n",
    "data_container.probs_ax.set_title(\"Classifier's probabilities over test set\")\n",
    "data_container.probs_ax.set_xlabel(\"x\")\n",
    "data_container.probs_ax.set_ylabel(\"prob(y(x)=1)\")\n",
    "\n",
    "data_container.roc_ax.set_title(\"ROC curves\")\n",
    "data_container.roc_ax.set_xlabel(\"fpr\")\n",
    "data_container.roc_ax.set_ylabel(\"tpr\")\n",
    "\n",
    "def onpick(event):\n",
    "    print(\"in onpick\")\n",
    "    # on the pick event, find the orig line corresponding to the\n",
    "    # legend proxy line, and toggle the visibility\n",
    "    # print(\"Just entered onpick!!\")\n",
    "    legline = event.artist\n",
    "    origlines = lined[legline]\n",
    "    vis = not origlines[0].get_visible()\n",
    "    for origline in origlines:\n",
    "        origline.set_visible(vis)\n",
    "    # Change the alpha on the line in the legend so we can see what lines\n",
    "    # have been toggled\n",
    "    if vis:\n",
    "        legline.set_alpha(1.0)\n",
    "    else:\n",
    "        legline.set_alpha(0.2)\n",
    "    probs_fig.canvas.draw()\n",
    "\n",
    "probs_fig.canvas.mpl_connect('pick_event', onpick)    \n",
    "data_container.probs_fig = probs_fig\n",
    "\n",
    "#Overlay the empirical probabilities (taken from the actual \"real\" distributions).\n",
    "def calc_bin_prob(bn_df):\n",
    "#     set_trace()\n",
    "    bin_x = bn_df[0].left\n",
    "    df = bn_df[1]\n",
    "    prob_1 = np.nan if len(df) == 0 else df['y'].sum()/len(df) \n",
    "    return bin_x, prob_1\n",
    "\n",
    "X = data_container.X\n",
    "y = data_container.y\n",
    "X_y_df = pd.DataFrame({'X':X.T[0], 'y':y})\n",
    "#Extend range otherwise inerval may be nan.\n",
    "X_y_df['x_binned'] = pd.cut(X_y_df['X'], bins=np.arange(X.min()-0.05, X.max() + 0.05, 0.05))\n",
    "x_binned = []\n",
    "prob_one_binned = []\n",
    "for bn_df in X_y_df.groupby('x_binned'):\n",
    "    bin_x, prob_1 = calc_bin_prob(bn_df)\n",
    "    x_binned.append(bin_x)\n",
    "    prob_one_binned.append(prob_1)\n",
    "prob_one_binned = np.array(prob_one_binned)\n",
    "data_container.probs_ax.plot(x_binned, prob_one_binned, 'r', label=\"empirical P(y=1)\")\n",
    "# data_container.probs_ax.plot(x_binned, prob_one_binned/(1-prob_one_binned), 'r+', label=\"empirical P(y=1)/P(y=0)\")\n",
    "# data_container.probs_ax.plot(x_binned, np.log(prob_one_binned/(1-prob_one_binned)), 'rx', label=\"empirical log(P(y=1)/P(y=0))\")\n",
    "\n",
    "data_container.probs_ax.legend()\n",
    "#Roc\n",
    "x_test_binned = X_y_df.set_index('X').loc[X_test.T[0],'x_binned'].values\n",
    "# if np.nan in x_test_binned:\n",
    "#     print(\"Found NAN!!\")\n",
    "#     for i,intr in enumerate(x_test_binned):\n",
    "#         if isinstance(intr, float):\n",
    "#             print(f\"The nan is at index {i}, corresponding to x_test {X_test[i]}\")\n",
    "def get_left(interval):\n",
    "    return interval.left\n",
    "\n",
    "\n",
    "x_test_binned = list(map(get_left , x_test_binned))\n",
    "empirical_probs = pd.DataFrame({'probs':prob_one_binned}, index = x_binned).loc[x_test_binned]\n",
    "fpr, tpr, thresh = roc_curve(y_test, empirical_probs)\n",
    "emp_auc = roc_auc_score(y_test, empirical_probs)\n",
    "data_container.roc_ax.plot(fpr, tpr, 'r', label=\"empirical, auc:{:0.2f}\".format(emp_auc))\n",
    "data_container.roc_ax.legend()\n",
    "#TODO: calc recall, precision, F1 over the empirical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now, let's apply a standard classification and assess the results.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1017\n",
      "           1       0.94      0.98      0.96       983\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.96      0.96      0.96      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[964  19]\n",
      " [ 63 954]]\n",
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x120cedfd0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x143fc0bd0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "X_train = data_container.X_train\n",
    "y_train = data_container.y_train\n",
    "X_test = data_container.X_test\n",
    "y_test = data_container.y_test\n",
    "\n",
    "clf_lr = LogisticRegressionCV(cv=5, random_state=4).fit(X_train, y_train)\n",
    "y_hat_lr = clf_lr.predict(X_test)\n",
    "prediction_probs = classification_assessment(X_test, y_test, y_hat_lr, clf_lr, data_container)\n",
    "# %matplotlib qt\n",
    "data_container.probs_ax.scatter(X_test, prediction_probs, s=2, label=f\"Baseline logistic\")\n",
    "\n",
    "\n",
    "data_container.probs_ax.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Next, we're going to dilute the '1' class to obtain an imbalanced dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a625e9345433d8d6a4e73e5b9346a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='dilute_factor', max=1.0, min=0.01, step=0.01), Buttâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@interact_manual\n",
    "def dilute_ones( dilute_factor=widgets.FloatSlider(min=0.01,max=1,step=0.01,value=0.01)):\n",
    "    X = data_container.X\n",
    "    y = data_container.y\n",
    "    #Dilute the 1's class\n",
    "    diluted_X, diluted_y = dilute_class(X, y, 1, dilute_factor)\n",
    "    data_container.one_dilute_factor = dilute_factor\n",
    "    data_container.diluted_X = diluted_X\n",
    "    data_container.diluted_y = diluted_y\n",
    "    #Dilute the train set separately, since we want to evaluate over the original test set, \n",
    "    # so we don't want any of it to be used for training. \n",
    "    diluted_X_train, diluted_y_train = dilute_class(data_container.X_train, data_container.y_train, 1, dilute_factor)\n",
    "    data_container.diluted_X_train_ones = diluted_X_train\n",
    "    data_container.diluted_y_train_ones = diluted_y_train\n",
    "    \n",
    "    \n",
    "    diluted_X_zeros = diluted_X[diluted_y == 0]\n",
    "    diluted_X_ones = diluted_X[diluted_y == 1]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=diluted_X_zeros.T[0], nbinsx=1000, name=\"zeros\"))\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_ones.T[0], nbinsx=1000, name=\"ones\"))\n",
    "    fig.show()\n",
    "    \n",
    "    hist_ax = data_container.probs_ax.twinx()\n",
    "    hist_ax.hist(diluted_X_zeros.T[0], bins=1000, alpha=0.1, color='b')\n",
    "    hist_ax.hist(diluted_X_ones.T[0], bins=1000, alpha=0.1, color='r')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diluted_clf = LogisticRegressionCV(cv=5, random_state=0).fit(data_container.diluted_X, data_container.diluted_y)\n",
    "# print(\"Before correction: Theta0_D: {}, theta1_D: {}\".format(diluted_clf.intercept_, diluted_clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Now, let's examine several methods to cope with the imbalance. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ba6fdb6eb24963bd548da1163d8c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='zero_dilute_factor', max=1.0, min=0.01, step=0.01), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "#Predict on diluted data, optionally dilute 0 class as well.\n",
    "def generate_corrected_clf(X, y, correction_method, tau, y_bar):\n",
    "    \n",
    "    if correction_method == 'none':\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "    #Apply correction\n",
    "    if correction_method == \"prior\":\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "        corrected_intercept = corrected_clf.intercept_ - np.log( (1-tau)/tau*y_bar/(1-y_bar)    )\n",
    "        corrected_clf.intercept_ = corrected_intercept\n",
    "       \n",
    "    if correction_method == \"weighting\":\n",
    "        w1 = tau/y_bar\n",
    "        w0 = (1-tau)/(1-y_bar)\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0, class_weight={0:w0, 1:w1}).fit(X, y)\n",
    "    if correction_method == 'gev':\n",
    "        corrected_clf = GevRegressionCV().fit(X,y)\n",
    "    return corrected_clf\n",
    "        \n",
    "@interact_manual\n",
    "def dilute_zeros_and_predict( zero_dilute_factor=widgets.FloatSlider(min=0.01,max=1,step=0.01,value=1), \n",
    "                           correction_method = ['none', 'prior', 'weighting', 'gev']):\n",
    "    diluted_X = data_container.diluted_X\n",
    "    diluted_y = data_container.diluted_y\n",
    "    #Dilute the 0's class\n",
    "   \n",
    "    diluted_1_0_X, diluted_1_0_y = dilute_class(diluted_X, diluted_y, 0, zero_dilute_factor)\n",
    "    data_container.diluted_X_ones_and_zeros = diluted_1_0_X\n",
    "    data_container.diluted_y_ones_and_zeros = diluted_1_0_y\n",
    "    #Dilute the train set separately, since we want to evaluate over the original test set, \n",
    "    # so we don't want any of it to be used for training. \n",
    "    diluted_X_train_ones_and_zeros, diluted_y_train_ones_and_zeros =\\\n",
    "        dilute_class(data_container.diluted_X_train_ones, data_container.diluted_y_train_ones, 0, zero_dilute_factor)\n",
    "    data_container.diluted_X_train_ones_and_zeros = diluted_X_train_ones_and_zeros\n",
    "    data_container.diluted_y_train_ones_and_zeros = diluted_y_train_ones_and_zeros\n",
    "    \n",
    "    \n",
    "    \n",
    "    diluted_X_zeros = diluted_1_0_X[diluted_1_0_y == 0]\n",
    "    diluted_X_ones = diluted_1_0_X[diluted_1_0_y == 1]\n",
    "    y = data_container.y\n",
    "    tau = sum(y)/len(y) #The actual population ratio, before any dilution.\n",
    "    y_bar =  sum(diluted_1_0_y)/len(diluted_1_0_y) #The sample ratio.\n",
    "    #Predict\n",
    "    display(HTML(f'<h3>Using a dataset of size {len(diluted_1_0_y)}, {len(diluted_X_zeros)} of which are 0,\\\n",
    "    and {len(diluted_X_ones)} are 1 <h3>'))\n",
    "    print(\"Before performing classification over the diluted data.\")\n",
    "    print(\"Original dataset: zeros: {}, ones: {}\".format(sum(data_container.y==0), sum(data_container.y == 1)))\n",
    "    print(\"Diluted training dataset: zeros: {}, ones: {}\".format(sum(data_container.diluted_y_train_ones_and_zeros==0), \n",
    "                                                        sum(data_container.diluted_y_train_ones_and_zeros == 1)))\n",
    "    \n",
    "    corrected_clf = generate_corrected_clf(data_container.diluted_X_train_ones_and_zeros, \n",
    "                                           data_container.diluted_y_train_ones_and_zeros, \n",
    "                                           correction_method, tau, y_bar)\n",
    "    X_test = data_container.X_test\n",
    "    y_test = data_container.y_test\n",
    "    y_hat = corrected_clf.predict(X_test)\n",
    "    prediction_probs = classification_assessment(X_test, data_container.y_test, \n",
    "                                                 y_hat, corrected_clf, data_container)\n",
    "#     print(prediction_probs)\n",
    "#     print(prediction_probs.shape)\n",
    "#     print(sum(prediction_probs > 0.5))\n",
    "    probs = corrected_clf.predict_proba(X_test)\n",
    "    data_container.probs = prediction_probs\n",
    "#     print(probs)\n",
    "#     print(\"######################{}###\".format(any(probs[:,1] != prediction_probs )))\n",
    "    data_container.probs_ax.scatter(X_test, probs[:,1], s=2, \n",
    "                                    label=\"0:{}%_1:{}%_corr:{}\".format(\n",
    "                                    int(zero_dilute_factor*100), int(data_container.one_dilute_factor*100), \n",
    "                                        correction_method))\n",
    "    data_container.probs_ax.legend()\n",
    "#     lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, probs[:,1])\n",
    "#     print(sum(probs[:,1]>0.5))\n",
    "#     print(lr_tpr)\n",
    "#     data_container.roc_ax.scatter(lr_fpr, lr_tpr)\n",
    "# #     probs_fig = go.Figure()\n",
    "#     probs_fig.add_trace(go.Scatter( x= X_test.T, y=probs[:,1].T ))\n",
    "#     probs_fig.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_zeros.T[0], nbinsx=1000))\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_ones.T[0], nbinsx=1000))\n",
    "    fig.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)\n",
    "probs = np.array(0.5*np.random.rand(len(y_test)))\n",
    "sum(probs>0.5)\n",
    "lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, probs)\n",
    "print(lr_thresh)\n",
    "plt.plot(lr_fpr, lr_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diluted_X = data_container.diluted_X\n",
    "diluted_y = data_container.diluted_y\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0).fit(diluted_X, diluted_y)\n",
    "y_hat_tmp = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "print(sum(probs>0.5))\n",
    "fpr, tpr, thresh = roc_curve(y_test, y_hat_tmp)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GEV #######################\n",
    "# import scipy.optimize.fmin_cg\n",
    "def gev(x, tau):\n",
    "    return np.exp( -np.power(1 + tau*x, -1/tau))\n",
    "\n",
    "\n",
    "class GevRegressionCV():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.curr_iter_x_dot_theta = None\n",
    "        self.curr_iter_pi = None\n",
    "    \n",
    "    @staticmethod \n",
    "    def err_func(theta_tau, X, y, reg_param, gev_instance):\n",
    "        x_dot_theta = np.dot(X, theta)\n",
    "        pi = gev(x_dot_theta)\n",
    "        h_theta = -1/len(y) *( np.dot(y, np.log(pi)) + np.dot(1-y, np.log(1 - pi))) + \n",
    "            reg_param/(2*len(y))*np.dot(theta_tau, theta_tau)\n",
    "        #Save for the gradient invocation, to save repeated evaluation. \n",
    "#         gev_instance.curr_iter_x_dot_theta =  x_dot_theta\n",
    "#         gev_instance.curr_iter_pi = pi\n",
    "        print(f\"h_theta: {h_theta}\")\n",
    "        return h_theta\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad(theta_tau, X, y, reg_param, gev_instance):\n",
    "        #These two are calculated twice. Make this more efficient later if necessary. \n",
    "        x_dot_theta = np.dot(X, theta)\n",
    "        pi = gev(x_dot_theta)\n",
    "#         x_dot_theta = gev_instance.curr_iter_x_dot_theta\n",
    "#         pi = gev_instance.curr_iter_pi\n",
    "        mult_vec = np.log(pi)*(y-pi)/( (1+tau*x_dot_theta)*(1-pi) )\n",
    "        grad_theta = -np.dot(X.T, mult_vec)\n",
    "        #Regularization\n",
    "        grad_theta[1:] += reg_param/len(y)*theta_tau[1:]\n",
    "        \n",
    "        u = 1/(tau*tau)*np.log((1+tau*x_dot_theta)) - x_dot_theta/( tau*(1+tau*x_dot_theta))\n",
    "        v = (y-pi)*np.log(pi)/(1-pi)\n",
    "        grad_tau = -np.dot(u,v) #TODO: regularize tau as well?\n",
    "        \n",
    "        return h_theta, np.append([grad_theta, grad_tau])\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        #This function requires two separate functions calls - one for the log-likelihood, \n",
    "        # and the other for the gradient calculation. I'm saving the \n",
    "        #Add intercept\n",
    "        first_col = np.array([np.ones(X.shape[0])])\n",
    "        X = np.concatenate((b.T, X), axis=1)\n",
    "        initial_guess\n",
    "        opt_theta_tau = scipy.optimize.fmin_cg(self.err_func, initial_guess,\n",
    "                                               fprime=self.grad, args=(X, y, self) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blobs test\n",
    "num_samples = 200000\n",
    "center_zero = 0\n",
    "center_one_1 = 2\n",
    "center_one_2 = -3\n",
    "std_zero = 0.7\n",
    "std_one = 0.6\n",
    "    \n",
    "dataset1 =  make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one_1], \n",
    "                                                                                     [center_one_2]],\n",
    "                                cluster_std=[std_zero, std_one, std_one],  shuffle=False, random_state=4) \n",
    "     \n",
    "# dataset1 =  make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one_1]],\n",
    "#                             cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "X, y = dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[y==2]=1\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = X[y==1]\n",
    "X_zeros = X[y==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots()\n",
    "bins = np.arange(-5,4,0.05);\n",
    "ax.hist(X_ones, bins=bins, label='ones', alpha=.3, density=True)\n",
    "ax.hist(X_zeros, bins=bins, label='zeros', alpha=0.3, density=True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(200000, 2)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "XX = np.concatenate([X, X*X], axis=1)\n",
    "XX.shape\n",
    "X[0]*X[0] == XX[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(XX, y, test_size=0.2)\n",
    "clf = LogisticRegressionCV(cv=5, random_state=4).fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "probs.shape\n",
    "X_test.shape\n",
    "ax.scatter(X_test, probs, color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([clf.intercept_, clf.coef_])\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = np.concatenate([np.ones(len(X_test)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(np.arange(-2,2,0.05))\n",
    "y = np.log(1+np.exp(2*x))\n",
    "plt.plot(x,y, x , np.log(np.exp(2*x           )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14aca3390>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fpr, lr_tpr, lr_thresh = roc_curve(data_container.y_test, data_container.probs)\n",
    "plt.figure()\n",
    "# plt.scatter(data_container.X_test, data_container.probs)\n",
    "plt.plot(lr_fpr, lr_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1455b0910>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13ab84f90>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(data_container.X_test, data_container.y_test)\n",
    "plt.scatter(data_container.X_test, data_container.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6405"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr_tpr)\n",
    "#plt.scatter(data_container.X_test, lr_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ad33c90>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(lr_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1359b7050>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "precision, recall, thresholds = precision_recall_curve( data_container.y_test, data_container.probs)\n",
    "plt.plot(  recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58427"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(thresholds)\n",
    "len(np.unique(data_container.probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, ..., 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
