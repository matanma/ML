{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U -q ipywidgets\n",
    "# !pip3 install -U -q pyarrow\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Data Science Helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import random \n",
    "\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline(connected=True)\n",
    "cf.set_config_file(colorscale='plotly', world_readable=True)\n",
    "\n",
    "# Extra options\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_columns = 25\n",
    "\n",
    "# Show all code cells outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import os\n",
    "from IPython.display import Image, display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from sklearn.base import clone\n",
    " \n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some utility functions.\n",
    "\n",
    "class DataContainer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_x(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def set_y(self, y):\n",
    "        self.y = y\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "def classification_assessment(X_test, y_test, y_test_predicted, clf, data_container=None):\n",
    "    print(classification_report(y_test, y_test_predicted))\n",
    "    cnf_matrix = confusion_matrix(y_test, y_test_predicted, labels=[1,0])\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=['incident=1','no incident=0'],normalize= False, \n",
    "                          title='Confusion matrix')\n",
    "    ns_probs = [0 for _ in range(len(y_test))]\n",
    "\n",
    "    # plot ROC\n",
    "    if data_container is None:\n",
    "        plt.figure()\n",
    "    lr_probs = clf.predict_proba(X_test)\n",
    "    # keep probabilities for the positive outcome only\n",
    "    lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "    lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "#     print(lr_probs)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, ns_thresh = roc_curve(y_test, ns_probs)\n",
    "    lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    if data_container is None:\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "        # axis labels\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        # show the legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "    else: \n",
    "        roc_ax = data_container.roc_ax\n",
    "        roc_ax.plot(lr_fpr, lr_tpr, marker='.', label='Logistic, auc:{:0.2f}'.format(lr_auc))\n",
    "        roc_ax.legend()\n",
    "    return lr_probs\n",
    "    \n",
    "    \n",
    "def dilute_class(X, y, class_tag, dilute_factor):\n",
    "    y_indices = y==class_tag\n",
    "    num_class_entries = sum(y_indices)\n",
    "    diluted_ys_indices =  random.sample(range(num_class_entries), int(num_class_entries*dilute_factor))\n",
    "    diluted_ys = y[y_indices][diluted_ys_indices]\n",
    "    diluted_X_other_classes = X[~y_indices]\n",
    "    diluted_X_class = X[y_indices][diluted_ys_indices]\n",
    "    diluted_X = np.concatenate([diluted_X_other_classes, diluted_X_class])\n",
    "    diluted_y = np.concatenate([ y[~y_indices], diluted_ys])\n",
    "    \n",
    "    return diluted_X, diluted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Single feature, two normal distributions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bd8dab855d40f38002c4b8b7b3d6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='num_samples', options=(500000, 10000, 1000000), value=500000), Droâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Choosing the initial dataset.\n",
    "#See https://plotly.com/python/histograms/\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "data_container = DataContainer()\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "\n",
    "@interact_manual\n",
    "def choose_dataset( num_samples=[500000, 10000, 1000000], \n",
    "                    ones_distribution=['normal', 'gev'],\n",
    "                    center_one=widgets.FloatSlider(min=-2,max=-0.5,step=0.1,value=-1),\n",
    "                    center_zero=widgets.FloatSlider(min=0.5,max=2,step=0.1,value=1),\n",
    "                    std_one=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.8),\n",
    "                    std_zero=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.5)\n",
    "                  ):\n",
    "    display(HTML(f'<h2>Plotting dataset of size {num_samples} <h2>'))\n",
    "    dataset_artificial_balanced_1_feature = \\\n",
    "                            make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one]],\n",
    "                                cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "    X, y = dataset_artificial_balanced_1_feature\n",
    "    if ones_distribution == 'gev':\n",
    "#         X_ones = np.random.exponential(10, sum(y))\n",
    "        X_ones = genextreme.rvs(c=1/300, loc=center_one, scale=std_one, size=sum(y))\n",
    "# u = np.random.rand(100000)\n",
    "# lmbd=1\n",
    "# k=1.5\n",
    "# weibull = lmbd*np.power((-np.log(1-u)),1/k) \n",
    "        \n",
    "        print(X_ones.shape)\n",
    "        print(X.shape)\n",
    "        X[y==1] = np.array([X_ones]).T\n",
    "        \n",
    "   \n",
    "    data_container.set_x(X)\n",
    "    data_container.set_y(y)\n",
    "#     print(\"X shape: {}\".format(X.shape))\n",
    "#     print(\"y shape: {}\".format(y.shape))\n",
    "    X_ones = X[y==1].T[0]\n",
    "    X_zeros = X[y==0].T[0]\n",
    "    print(\"1: mean: {:0.3f}, std: {:0.3f} \\n0: mean: {:0.3f}, std: {:0.3f} \".format(X_ones.mean(), X_ones.std(), X_zeros.mean(), X_zeros.std()))\n",
    " \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=X_zeros, nbinsx=500, histnorm='probability density', opacity=0.7, name='zeros'))\n",
    "    fig.add_trace(go.Histogram(x=X_ones, nbinsx=500, histnorm='probability density', opacity=0.7, name='ones'))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Single feature, three normal distributions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split ones dataset\n",
    "\n",
    "@interact_manual\n",
    "def choose_dataset_split( zero_nsamples=[500000, 10000, 1000000], \n",
    "                          one_1_nsamples=[500000, 10000, 1000000], \n",
    "                          one_2_nsamples=[500000, 20000, 50000],\n",
    "                    center_zero=widgets.FloatSlider(min=-1,max=1,step=0.1,value=0),\n",
    "                    center_one_1=widgets.FloatSlider(min=1,max=3,step=0.1,value=2),\n",
    "                    center_one_2=widgets.FloatSlider(min=-4,max=-2,step=0.1,value=-3),\n",
    "                    std_zero=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.8),\n",
    "                    std_one_1=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.6),\n",
    "                    std_one_2=widgets.FloatSlider(min=0.2,max=2,step=0.1,value=0.4)\n",
    "                  ):\n",
    "    display(HTML(f'<h2>Plotting dataset of size {num_samples} <h2>'))\n",
    "\n",
    "    dataset_split =  make_blobs(n_samples=[zero_nsamples, one_1_nsamples, one_2_nsamples ],\n",
    "                                n_features=1, centers=[[center_zero], [center_one_1],[center_one_2]],\n",
    "                                    cluster_std=[std_zero, std_one_1, std_one_2],  shuffle=False, random_state=4)\n",
    "\n",
    "#     dataset_artificial_balanced_1_feature = \\\n",
    "#                             make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one]],\n",
    "#                                 cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "    X, y = dataset_split\n",
    "    y[y==2] = 1\n",
    "    X = np.concatenate([X, X*X], axis=1)\n",
    "    data_container.set_x(X)\n",
    "    data_container.set_y(y)\n",
    "    X_ones = X[y==1].T[0]\n",
    "    X_zeros = X[y==0].T[0]\n",
    "    print(\"1: mean: {:0.3f}, std: {:0.3f} \\n0: mean: {:0.3f}, std: {:0.3f} \".format(X_ones.mean(), X_ones.std(), X_zeros.mean(), X_zeros.std()))\n",
    " \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=X_zeros, nbinsx=500, histnorm='probability density', opacity=0.7, name='zeros'))\n",
    "    fig.add_trace(go.Histogram(x=X_ones, nbinsx=500, histnorm='probability density', opacity=0.7, name='ones'))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Single feature, GEV</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3990000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:23: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in power\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12eaac250>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1398be850>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1398be890>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1398bea90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1280x960 with 0 Axes>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([    0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "           36.,   132.,   236.,   320.,   434.,   565.,   706.,   864.,\n",
       "         1007.,  1207.,  1340.,  1458.,  1697.,  1834.,  2017.,  2248.,\n",
       "         2353.,  2612.,  2848.,  3016.,  3142.,  3346.,  3550.,  3724.,\n",
       "         3922.,  3987.,  4241.,  4468.,  4609.,  4809.,  4965.,  5054.,\n",
       "         5220.,  5474.,  5593.,  5657.,  5901.,  6129.,  6157.,  6320.,\n",
       "         6460.,  6563.,  6682.,  6823.,  6974.,  7132.,  7253.,  7231.,\n",
       "         7358.,  7520.,  7559.,  7750.,  7846.,  7871.,  8048.,  8133.,\n",
       "         8193.,  8284.,  8268.,  8408.,  8464.,  8594.,  8665.,  8762.,\n",
       "         8780.,  8805.,  8846.,  8870.,  8954.,  9018.,  9085.,  9124.,\n",
       "         9144.,  9223.,  9210.,  9273.,  9318.,  9329.,  9356.,  9437.,\n",
       "         9420.,  9488.,  9528.,  9483.,  9586.,  9561.,  9599.,  9627.,\n",
       "         9617.,  9672.,  9684.,  9689.,  9706.,  9712.,  9735.,  9784.,\n",
       "         9768.,  9784.,  9798.,  9831.,  9832.,  9818.,  9822.,  9847.,\n",
       "         9854.,  9870.,  9861.,  9888.,  9893.,  9888.,  9909.,  9910.,\n",
       "         9924.,  9915.,  9920.,  9927.,  9926.,  9953.,  9944.,  9962.,\n",
       "         9945.,  9950.,  9961.,  9960.,  9966.,  9969.,  9963.,  9963.,\n",
       "         9967.,  9969.,  9975.,  9961.,  9973.,  9973.,  9982.,  9987.,\n",
       "         9981.,  9986.,  9983.,  9984.,  9989.,  9989.,  9990.,  9988.,\n",
       "         9995.,  9994.,  9992.,  9994.,  9987.,  9995.,  9998.,  9994.,\n",
       "         9997.,  9997.,  9992.,  9998.,  9995.,  9997.,  9999.,  9996.,\n",
       "         9997.,  9995.,  9998.,  9997.,  9989.,  9999.,  9999.,  9999.,\n",
       "         9997.,  9998.,  9998.,  9999.,  9998.,  9999., 10000.,  9998.,\n",
       "         9998.,  9999., 10000.,  9999., 10000., 10000.,  9999.,  9999.,\n",
       "        10000., 10000., 10000., 10000.,  9999., 10000.,  9999., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000., 10000.,\n",
       "        10000., 10000., 10000., 10000., 10000., 10000., 10000.]),\n",
       " array([-1.95000000e+00, -1.90000000e+00, -1.85000000e+00, -1.80000000e+00,\n",
       "        -1.75000000e+00, -1.70000000e+00, -1.65000000e+00, -1.60000000e+00,\n",
       "        -1.55000000e+00, -1.50000000e+00, -1.45000000e+00, -1.40000000e+00,\n",
       "        -1.35000000e+00, -1.30000000e+00, -1.25000000e+00, -1.20000000e+00,\n",
       "        -1.15000000e+00, -1.10000000e+00, -1.05000000e+00, -1.00000000e+00,\n",
       "        -9.50000000e-01, -9.00000000e-01, -8.50000000e-01, -8.00000000e-01,\n",
       "        -7.50000000e-01, -7.00000000e-01, -6.50000000e-01, -6.00000000e-01,\n",
       "        -5.50000000e-01, -5.00000000e-01, -4.50000000e-01, -4.00000000e-01,\n",
       "        -3.50000000e-01, -3.00000000e-01, -2.50000000e-01, -2.00000000e-01,\n",
       "        -1.50000000e-01, -1.00000000e-01, -5.00000000e-02,  1.77635684e-15,\n",
       "         5.00000000e-02,  1.00000000e-01,  1.50000000e-01,  2.00000000e-01,\n",
       "         2.50000000e-01,  3.00000000e-01,  3.50000000e-01,  4.00000000e-01,\n",
       "         4.50000000e-01,  5.00000000e-01,  5.50000000e-01,  6.00000000e-01,\n",
       "         6.50000000e-01,  7.00000000e-01,  7.50000000e-01,  8.00000000e-01,\n",
       "         8.50000000e-01,  9.00000000e-01,  9.50000000e-01,  1.00000000e+00,\n",
       "         1.05000000e+00,  1.10000000e+00,  1.15000000e+00,  1.20000000e+00,\n",
       "         1.25000000e+00,  1.30000000e+00,  1.35000000e+00,  1.40000000e+00,\n",
       "         1.45000000e+00,  1.50000000e+00,  1.55000000e+00,  1.60000000e+00,\n",
       "         1.65000000e+00,  1.70000000e+00,  1.75000000e+00,  1.80000000e+00,\n",
       "         1.85000000e+00,  1.90000000e+00,  1.95000000e+00,  2.00000000e+00,\n",
       "         2.05000000e+00,  2.10000000e+00,  2.15000000e+00,  2.20000000e+00,\n",
       "         2.25000000e+00,  2.30000000e+00,  2.35000000e+00,  2.40000000e+00,\n",
       "         2.45000000e+00,  2.50000000e+00,  2.55000000e+00,  2.60000000e+00,\n",
       "         2.65000000e+00,  2.70000000e+00,  2.75000000e+00,  2.80000000e+00,\n",
       "         2.85000000e+00,  2.90000000e+00,  2.95000000e+00,  3.00000000e+00,\n",
       "         3.05000000e+00,  3.10000000e+00,  3.15000000e+00,  3.20000000e+00,\n",
       "         3.25000000e+00,  3.30000000e+00,  3.35000000e+00,  3.40000000e+00,\n",
       "         3.45000000e+00,  3.50000000e+00,  3.55000000e+00,  3.60000000e+00,\n",
       "         3.65000000e+00,  3.70000000e+00,  3.75000000e+00,  3.80000000e+00,\n",
       "         3.85000000e+00,  3.90000000e+00,  3.95000000e+00,  4.00000000e+00,\n",
       "         4.05000000e+00,  4.10000000e+00,  4.15000000e+00,  4.20000000e+00,\n",
       "         4.25000000e+00,  4.30000000e+00,  4.35000000e+00,  4.40000000e+00,\n",
       "         4.45000000e+00,  4.50000000e+00,  4.55000000e+00,  4.60000000e+00,\n",
       "         4.65000000e+00,  4.70000000e+00,  4.75000000e+00,  4.80000000e+00,\n",
       "         4.85000000e+00,  4.90000000e+00,  4.95000000e+00,  5.00000000e+00,\n",
       "         5.05000000e+00,  5.10000000e+00,  5.15000000e+00,  5.20000000e+00,\n",
       "         5.25000000e+00,  5.30000000e+00,  5.35000000e+00,  5.40000000e+00,\n",
       "         5.45000000e+00,  5.50000000e+00,  5.55000000e+00,  5.60000000e+00,\n",
       "         5.65000000e+00,  5.70000000e+00,  5.75000000e+00,  5.80000000e+00,\n",
       "         5.85000000e+00,  5.90000000e+00,  5.95000000e+00,  6.00000000e+00,\n",
       "         6.05000000e+00,  6.10000000e+00,  6.15000000e+00,  6.20000000e+00,\n",
       "         6.25000000e+00,  6.30000000e+00,  6.35000000e+00,  6.40000000e+00,\n",
       "         6.45000000e+00,  6.50000000e+00,  6.55000000e+00,  6.60000000e+00,\n",
       "         6.65000000e+00,  6.70000000e+00,  6.75000000e+00,  6.80000000e+00,\n",
       "         6.85000000e+00,  6.90000000e+00,  6.95000000e+00,  7.00000000e+00,\n",
       "         7.05000000e+00,  7.10000000e+00,  7.15000000e+00,  7.20000000e+00,\n",
       "         7.25000000e+00,  7.30000000e+00,  7.35000000e+00,  7.40000000e+00,\n",
       "         7.45000000e+00,  7.50000000e+00,  7.55000000e+00,  7.60000000e+00,\n",
       "         7.65000000e+00,  7.70000000e+00,  7.75000000e+00,  7.80000000e+00,\n",
       "         7.85000000e+00,  7.90000000e+00,  7.95000000e+00,  8.00000000e+00,\n",
       "         8.05000000e+00,  8.10000000e+00,  8.15000000e+00,  8.20000000e+00,\n",
       "         8.25000000e+00,  8.30000000e+00,  8.35000000e+00,  8.40000000e+00,\n",
       "         8.45000000e+00,  8.50000000e+00,  8.55000000e+00,  8.60000000e+00,\n",
       "         8.65000000e+00,  8.70000000e+00,  8.75000000e+00,  8.80000000e+00,\n",
       "         8.85000000e+00,  8.90000000e+00,  8.95000000e+00,  9.00000000e+00,\n",
       "         9.05000000e+00,  9.10000000e+00,  9.15000000e+00,  9.20000000e+00,\n",
       "         9.25000000e+00,  9.30000000e+00,  9.35000000e+00,  9.40000000e+00,\n",
       "         9.45000000e+00,  9.50000000e+00,  9.55000000e+00,  9.60000000e+00,\n",
       "         9.65000000e+00,  9.70000000e+00,  9.75000000e+00,  9.80000000e+00,\n",
       "         9.85000000e+00,  9.90000000e+00,  9.95000000e+00,  1.00000000e+01,\n",
       "         1.00500000e+01,  1.01000000e+01,  1.01500000e+01,  1.02000000e+01,\n",
       "         1.02500000e+01,  1.03000000e+01,  1.03500000e+01,  1.04000000e+01,\n",
       "         1.04500000e+01,  1.05000000e+01,  1.05500000e+01,  1.06000000e+01,\n",
       "         1.06500000e+01,  1.07000000e+01,  1.07500000e+01,  1.08000000e+01,\n",
       "         1.08500000e+01,  1.09000000e+01,  1.09500000e+01,  1.10000000e+01,\n",
       "         1.10500000e+01,  1.11000000e+01,  1.11500000e+01,  1.12000000e+01,\n",
       "         1.12500000e+01,  1.13000000e+01,  1.13500000e+01,  1.14000000e+01,\n",
       "         1.14500000e+01,  1.15000000e+01,  1.15500000e+01,  1.16000000e+01,\n",
       "         1.16500000e+01,  1.17000000e+01,  1.17500000e+01,  1.18000000e+01,\n",
       "         1.18500000e+01,  1.19000000e+01,  1.19500000e+01,  1.20000000e+01,\n",
       "         1.20500000e+01,  1.21000000e+01,  1.21500000e+01,  1.22000000e+01,\n",
       "         1.22500000e+01,  1.23000000e+01,  1.23500000e+01,  1.24000000e+01,\n",
       "         1.24500000e+01,  1.25000000e+01,  1.25500000e+01,  1.26000000e+01,\n",
       "         1.26500000e+01,  1.27000000e+01,  1.27500000e+01,  1.28000000e+01,\n",
       "         1.28500000e+01,  1.29000000e+01,  1.29500000e+01,  1.30000000e+01,\n",
       "         1.30500000e+01,  1.31000000e+01,  1.31500000e+01,  1.32000000e+01,\n",
       "         1.32500000e+01,  1.33000000e+01,  1.33500000e+01,  1.34000000e+01,\n",
       "         1.34500000e+01,  1.35000000e+01,  1.35500000e+01,  1.36000000e+01,\n",
       "         1.36500000e+01,  1.37000000e+01,  1.37500000e+01,  1.38000000e+01,\n",
       "         1.38500000e+01,  1.39000000e+01,  1.39500000e+01,  1.40000000e+01,\n",
       "         1.40500000e+01,  1.41000000e+01,  1.41500000e+01,  1.42000000e+01,\n",
       "         1.42500000e+01,  1.43000000e+01,  1.43500000e+01,  1.44000000e+01,\n",
       "         1.44500000e+01,  1.45000000e+01,  1.45500000e+01,  1.46000000e+01,\n",
       "         1.46500000e+01,  1.47000000e+01,  1.47500000e+01,  1.48000000e+01,\n",
       "         1.48500000e+01,  1.49000000e+01,  1.49500000e+01,  1.50000000e+01,\n",
       "         1.50500000e+01,  1.51000000e+01,  1.51500000e+01,  1.52000000e+01,\n",
       "         1.52500000e+01,  1.53000000e+01,  1.53500000e+01,  1.54000000e+01,\n",
       "         1.54500000e+01,  1.55000000e+01,  1.55500000e+01,  1.56000000e+01,\n",
       "         1.56500000e+01,  1.57000000e+01,  1.57500000e+01,  1.58000000e+01,\n",
       "         1.58500000e+01,  1.59000000e+01,  1.59500000e+01,  1.60000000e+01,\n",
       "         1.60500000e+01,  1.61000000e+01,  1.61500000e+01,  1.62000000e+01,\n",
       "         1.62500000e+01,  1.63000000e+01,  1.63500000e+01,  1.64000000e+01,\n",
       "         1.64500000e+01,  1.65000000e+01,  1.65500000e+01,  1.66000000e+01,\n",
       "         1.66500000e+01,  1.67000000e+01,  1.67500000e+01,  1.68000000e+01,\n",
       "         1.68500000e+01,  1.69000000e+01,  1.69500000e+01,  1.70000000e+01,\n",
       "         1.70500000e+01,  1.71000000e+01,  1.71500000e+01,  1.72000000e+01,\n",
       "         1.72500000e+01,  1.73000000e+01,  1.73500000e+01,  1.74000000e+01,\n",
       "         1.74500000e+01,  1.75000000e+01,  1.75500000e+01,  1.76000000e+01,\n",
       "         1.76500000e+01,  1.77000000e+01,  1.77500000e+01,  1.78000000e+01,\n",
       "         1.78500000e+01,  1.79000000e+01,  1.79500000e+01,  1.80000000e+01]),\n",
       " <a list of 399 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(array([1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04,\n",
       "        1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04,\n",
       "        1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04,\n",
       "        1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04,\n",
       "        1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04,\n",
       "        1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04,\n",
       "        1.000e+04, 1.000e+04, 1.000e+04, 1.000e+04, 9.964e+03, 9.868e+03,\n",
       "        9.764e+03, 9.680e+03, 9.566e+03, 9.435e+03, 9.294e+03, 9.136e+03,\n",
       "        8.993e+03, 8.793e+03, 8.660e+03, 8.542e+03, 8.303e+03, 8.166e+03,\n",
       "        7.983e+03, 7.752e+03, 7.647e+03, 7.388e+03, 7.152e+03, 6.984e+03,\n",
       "        6.858e+03, 6.654e+03, 6.450e+03, 6.276e+03, 6.078e+03, 6.013e+03,\n",
       "        5.759e+03, 5.532e+03, 5.391e+03, 5.191e+03, 5.035e+03, 4.946e+03,\n",
       "        4.780e+03, 4.526e+03, 4.407e+03, 4.343e+03, 4.099e+03, 3.871e+03,\n",
       "        3.843e+03, 3.680e+03, 3.540e+03, 3.437e+03, 3.318e+03, 3.177e+03,\n",
       "        3.026e+03, 2.868e+03, 2.747e+03, 2.769e+03, 2.642e+03, 2.480e+03,\n",
       "        2.441e+03, 2.250e+03, 2.154e+03, 2.129e+03, 1.952e+03, 1.867e+03,\n",
       "        1.807e+03, 1.716e+03, 1.732e+03, 1.592e+03, 1.536e+03, 1.406e+03,\n",
       "        1.335e+03, 1.238e+03, 1.220e+03, 1.195e+03, 1.154e+03, 1.130e+03,\n",
       "        1.046e+03, 9.820e+02, 9.150e+02, 8.760e+02, 8.560e+02, 7.770e+02,\n",
       "        7.900e+02, 7.270e+02, 6.820e+02, 6.710e+02, 6.440e+02, 5.630e+02,\n",
       "        5.800e+02, 5.120e+02, 4.720e+02, 5.170e+02, 4.140e+02, 4.390e+02,\n",
       "        4.010e+02, 3.730e+02, 3.830e+02, 3.280e+02, 3.160e+02, 3.110e+02,\n",
       "        2.940e+02, 2.880e+02, 2.650e+02, 2.160e+02, 2.320e+02, 2.160e+02,\n",
       "        2.020e+02, 1.690e+02, 1.680e+02, 1.820e+02, 1.780e+02, 1.530e+02,\n",
       "        1.460e+02, 1.300e+02, 1.390e+02, 1.120e+02, 1.070e+02, 1.120e+02,\n",
       "        9.100e+01, 9.000e+01, 7.600e+01, 8.500e+01, 8.000e+01, 7.300e+01,\n",
       "        7.400e+01, 4.700e+01, 5.600e+01, 3.800e+01, 5.500e+01, 5.000e+01,\n",
       "        3.900e+01, 4.000e+01, 3.400e+01, 3.100e+01, 3.700e+01, 3.700e+01,\n",
       "        3.300e+01, 3.100e+01, 2.500e+01, 3.900e+01, 2.700e+01, 2.700e+01,\n",
       "        1.800e+01, 1.300e+01, 1.900e+01, 1.400e+01, 1.700e+01, 1.600e+01,\n",
       "        1.100e+01, 1.100e+01, 1.000e+01, 1.200e+01, 5.000e+00, 6.000e+00,\n",
       "        8.000e+00, 6.000e+00, 1.300e+01, 5.000e+00, 2.000e+00, 6.000e+00,\n",
       "        3.000e+00, 3.000e+00, 8.000e+00, 2.000e+00, 5.000e+00, 3.000e+00,\n",
       "        1.000e+00, 4.000e+00, 3.000e+00, 5.000e+00, 2.000e+00, 3.000e+00,\n",
       "        1.100e+01, 1.000e+00, 1.000e+00, 1.000e+00, 3.000e+00, 2.000e+00,\n",
       "        2.000e+00, 1.000e+00, 2.000e+00, 1.000e+00, 0.000e+00, 2.000e+00,\n",
       "        2.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "        0.000e+00, 0.000e+00, 0.000e+00]),\n",
       " array([-1.95000000e+00, -1.90000000e+00, -1.85000000e+00, -1.80000000e+00,\n",
       "        -1.75000000e+00, -1.70000000e+00, -1.65000000e+00, -1.60000000e+00,\n",
       "        -1.55000000e+00, -1.50000000e+00, -1.45000000e+00, -1.40000000e+00,\n",
       "        -1.35000000e+00, -1.30000000e+00, -1.25000000e+00, -1.20000000e+00,\n",
       "        -1.15000000e+00, -1.10000000e+00, -1.05000000e+00, -1.00000000e+00,\n",
       "        -9.50000000e-01, -9.00000000e-01, -8.50000000e-01, -8.00000000e-01,\n",
       "        -7.50000000e-01, -7.00000000e-01, -6.50000000e-01, -6.00000000e-01,\n",
       "        -5.50000000e-01, -5.00000000e-01, -4.50000000e-01, -4.00000000e-01,\n",
       "        -3.50000000e-01, -3.00000000e-01, -2.50000000e-01, -2.00000000e-01,\n",
       "        -1.50000000e-01, -1.00000000e-01, -5.00000000e-02,  1.77635684e-15,\n",
       "         5.00000000e-02,  1.00000000e-01,  1.50000000e-01,  2.00000000e-01,\n",
       "         2.50000000e-01,  3.00000000e-01,  3.50000000e-01,  4.00000000e-01,\n",
       "         4.50000000e-01,  5.00000000e-01,  5.50000000e-01,  6.00000000e-01,\n",
       "         6.50000000e-01,  7.00000000e-01,  7.50000000e-01,  8.00000000e-01,\n",
       "         8.50000000e-01,  9.00000000e-01,  9.50000000e-01,  1.00000000e+00,\n",
       "         1.05000000e+00,  1.10000000e+00,  1.15000000e+00,  1.20000000e+00,\n",
       "         1.25000000e+00,  1.30000000e+00,  1.35000000e+00,  1.40000000e+00,\n",
       "         1.45000000e+00,  1.50000000e+00,  1.55000000e+00,  1.60000000e+00,\n",
       "         1.65000000e+00,  1.70000000e+00,  1.75000000e+00,  1.80000000e+00,\n",
       "         1.85000000e+00,  1.90000000e+00,  1.95000000e+00,  2.00000000e+00,\n",
       "         2.05000000e+00,  2.10000000e+00,  2.15000000e+00,  2.20000000e+00,\n",
       "         2.25000000e+00,  2.30000000e+00,  2.35000000e+00,  2.40000000e+00,\n",
       "         2.45000000e+00,  2.50000000e+00,  2.55000000e+00,  2.60000000e+00,\n",
       "         2.65000000e+00,  2.70000000e+00,  2.75000000e+00,  2.80000000e+00,\n",
       "         2.85000000e+00,  2.90000000e+00,  2.95000000e+00,  3.00000000e+00,\n",
       "         3.05000000e+00,  3.10000000e+00,  3.15000000e+00,  3.20000000e+00,\n",
       "         3.25000000e+00,  3.30000000e+00,  3.35000000e+00,  3.40000000e+00,\n",
       "         3.45000000e+00,  3.50000000e+00,  3.55000000e+00,  3.60000000e+00,\n",
       "         3.65000000e+00,  3.70000000e+00,  3.75000000e+00,  3.80000000e+00,\n",
       "         3.85000000e+00,  3.90000000e+00,  3.95000000e+00,  4.00000000e+00,\n",
       "         4.05000000e+00,  4.10000000e+00,  4.15000000e+00,  4.20000000e+00,\n",
       "         4.25000000e+00,  4.30000000e+00,  4.35000000e+00,  4.40000000e+00,\n",
       "         4.45000000e+00,  4.50000000e+00,  4.55000000e+00,  4.60000000e+00,\n",
       "         4.65000000e+00,  4.70000000e+00,  4.75000000e+00,  4.80000000e+00,\n",
       "         4.85000000e+00,  4.90000000e+00,  4.95000000e+00,  5.00000000e+00,\n",
       "         5.05000000e+00,  5.10000000e+00,  5.15000000e+00,  5.20000000e+00,\n",
       "         5.25000000e+00,  5.30000000e+00,  5.35000000e+00,  5.40000000e+00,\n",
       "         5.45000000e+00,  5.50000000e+00,  5.55000000e+00,  5.60000000e+00,\n",
       "         5.65000000e+00,  5.70000000e+00,  5.75000000e+00,  5.80000000e+00,\n",
       "         5.85000000e+00,  5.90000000e+00,  5.95000000e+00,  6.00000000e+00,\n",
       "         6.05000000e+00,  6.10000000e+00,  6.15000000e+00,  6.20000000e+00,\n",
       "         6.25000000e+00,  6.30000000e+00,  6.35000000e+00,  6.40000000e+00,\n",
       "         6.45000000e+00,  6.50000000e+00,  6.55000000e+00,  6.60000000e+00,\n",
       "         6.65000000e+00,  6.70000000e+00,  6.75000000e+00,  6.80000000e+00,\n",
       "         6.85000000e+00,  6.90000000e+00,  6.95000000e+00,  7.00000000e+00,\n",
       "         7.05000000e+00,  7.10000000e+00,  7.15000000e+00,  7.20000000e+00,\n",
       "         7.25000000e+00,  7.30000000e+00,  7.35000000e+00,  7.40000000e+00,\n",
       "         7.45000000e+00,  7.50000000e+00,  7.55000000e+00,  7.60000000e+00,\n",
       "         7.65000000e+00,  7.70000000e+00,  7.75000000e+00,  7.80000000e+00,\n",
       "         7.85000000e+00,  7.90000000e+00,  7.95000000e+00,  8.00000000e+00,\n",
       "         8.05000000e+00,  8.10000000e+00,  8.15000000e+00,  8.20000000e+00,\n",
       "         8.25000000e+00,  8.30000000e+00,  8.35000000e+00,  8.40000000e+00,\n",
       "         8.45000000e+00,  8.50000000e+00,  8.55000000e+00,  8.60000000e+00,\n",
       "         8.65000000e+00,  8.70000000e+00,  8.75000000e+00,  8.80000000e+00,\n",
       "         8.85000000e+00,  8.90000000e+00,  8.95000000e+00,  9.00000000e+00,\n",
       "         9.05000000e+00,  9.10000000e+00,  9.15000000e+00,  9.20000000e+00,\n",
       "         9.25000000e+00,  9.30000000e+00,  9.35000000e+00,  9.40000000e+00,\n",
       "         9.45000000e+00,  9.50000000e+00,  9.55000000e+00,  9.60000000e+00,\n",
       "         9.65000000e+00,  9.70000000e+00,  9.75000000e+00,  9.80000000e+00,\n",
       "         9.85000000e+00,  9.90000000e+00,  9.95000000e+00,  1.00000000e+01,\n",
       "         1.00500000e+01,  1.01000000e+01,  1.01500000e+01,  1.02000000e+01,\n",
       "         1.02500000e+01,  1.03000000e+01,  1.03500000e+01,  1.04000000e+01,\n",
       "         1.04500000e+01,  1.05000000e+01,  1.05500000e+01,  1.06000000e+01,\n",
       "         1.06500000e+01,  1.07000000e+01,  1.07500000e+01,  1.08000000e+01,\n",
       "         1.08500000e+01,  1.09000000e+01,  1.09500000e+01,  1.10000000e+01,\n",
       "         1.10500000e+01,  1.11000000e+01,  1.11500000e+01,  1.12000000e+01,\n",
       "         1.12500000e+01,  1.13000000e+01,  1.13500000e+01,  1.14000000e+01,\n",
       "         1.14500000e+01,  1.15000000e+01,  1.15500000e+01,  1.16000000e+01,\n",
       "         1.16500000e+01,  1.17000000e+01,  1.17500000e+01,  1.18000000e+01,\n",
       "         1.18500000e+01,  1.19000000e+01,  1.19500000e+01,  1.20000000e+01,\n",
       "         1.20500000e+01,  1.21000000e+01,  1.21500000e+01,  1.22000000e+01,\n",
       "         1.22500000e+01,  1.23000000e+01,  1.23500000e+01,  1.24000000e+01,\n",
       "         1.24500000e+01,  1.25000000e+01,  1.25500000e+01,  1.26000000e+01,\n",
       "         1.26500000e+01,  1.27000000e+01,  1.27500000e+01,  1.28000000e+01,\n",
       "         1.28500000e+01,  1.29000000e+01,  1.29500000e+01,  1.30000000e+01,\n",
       "         1.30500000e+01,  1.31000000e+01,  1.31500000e+01,  1.32000000e+01,\n",
       "         1.32500000e+01,  1.33000000e+01,  1.33500000e+01,  1.34000000e+01,\n",
       "         1.34500000e+01,  1.35000000e+01,  1.35500000e+01,  1.36000000e+01,\n",
       "         1.36500000e+01,  1.37000000e+01,  1.37500000e+01,  1.38000000e+01,\n",
       "         1.38500000e+01,  1.39000000e+01,  1.39500000e+01,  1.40000000e+01,\n",
       "         1.40500000e+01,  1.41000000e+01,  1.41500000e+01,  1.42000000e+01,\n",
       "         1.42500000e+01,  1.43000000e+01,  1.43500000e+01,  1.44000000e+01,\n",
       "         1.44500000e+01,  1.45000000e+01,  1.45500000e+01,  1.46000000e+01,\n",
       "         1.46500000e+01,  1.47000000e+01,  1.47500000e+01,  1.48000000e+01,\n",
       "         1.48500000e+01,  1.49000000e+01,  1.49500000e+01,  1.50000000e+01,\n",
       "         1.50500000e+01,  1.51000000e+01,  1.51500000e+01,  1.52000000e+01,\n",
       "         1.52500000e+01,  1.53000000e+01,  1.53500000e+01,  1.54000000e+01,\n",
       "         1.54500000e+01,  1.55000000e+01,  1.55500000e+01,  1.56000000e+01,\n",
       "         1.56500000e+01,  1.57000000e+01,  1.57500000e+01,  1.58000000e+01,\n",
       "         1.58500000e+01,  1.59000000e+01,  1.59500000e+01,  1.60000000e+01,\n",
       "         1.60500000e+01,  1.61000000e+01,  1.61500000e+01,  1.62000000e+01,\n",
       "         1.62500000e+01,  1.63000000e+01,  1.63500000e+01,  1.64000000e+01,\n",
       "         1.64500000e+01,  1.65000000e+01,  1.65500000e+01,  1.66000000e+01,\n",
       "         1.66500000e+01,  1.67000000e+01,  1.67500000e+01,  1.68000000e+01,\n",
       "         1.68500000e+01,  1.69000000e+01,  1.69500000e+01,  1.70000000e+01,\n",
       "         1.70500000e+01,  1.71000000e+01,  1.71500000e+01,  1.72000000e+01,\n",
       "         1.72500000e+01,  1.73000000e+01,  1.73500000e+01,  1.74000000e+01,\n",
       "         1.74500000e+01,  1.75000000e+01,  1.75500000e+01,  1.76000000e+01,\n",
       "         1.76500000e+01,  1.77000000e+01,  1.77500000e+01,  1.78000000e+01,\n",
       "         1.78500000e+01,  1.79000000e+01,  1.79500000e+01,  1.80000000e+01]),\n",
       " <a list of 399 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3990000, 1)\n",
      "y shape: (3990000,)\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer()\n",
    "mu = -2\n",
    "sigma = 2\n",
    "k = 1.5\n",
    "x = np.arange(mu+0.05, mu+10*sigma, 0.05);\n",
    "F = 1-np.exp(-np.power((-(mu-x)/sigma), k));\n",
    "num_repetitions = 10000\n",
    "xx = np.array([x]*num_repetitions);\n",
    "xx = xx.reshape(1, num_repetitions*len(x))[0];\n",
    "\n",
    "u1 = np.random.rand(len(xx));#\n",
    "# u2 = np.random.rand(len(x))\n",
    "F_inverse_u = xx - sigma*np.power((-np.log(1-u1)), 1/k);#\n",
    "\n",
    "print(F_inverse_u.shape);\n",
    "# F_inverse_u\n",
    "x_ones = xx[F_inverse_u >= 0];\n",
    "x_zeros = xx[F_inverse_u < 0];\n",
    "n1, bins1,_ = plt.hist(x_ones, bins=np.append(x, x[-1] + 0.05), color='r', label= \"ones\", alpha=0.5, density=False);\n",
    "n0, bins0,_ = plt.hist(x_zeros, bins=np.append(x, x[-1] + 0.05), color='b', label= \"zeros\", alpha=0.5, density=False);\n",
    "empirical_probs = list(map(lambda tup : tup[0]/(tup[0]+tup[1]), zip(n1,n0) ))\n",
    "\n",
    "F_0 = 1-np.exp(-np.power((-(-x)/sigma), k));\n",
    "plt.plot(x, F_0, color='y');\n",
    "plt.plot(x, empirical_probs, color='g');\n",
    "F_sig = 1/(1+np.exp(-x/(0.5*sigma)))\n",
    "plt.plot(x, F_sig, color='k');\n",
    "plt.legend();\n",
    "# plt.plot(x, F)\n",
    "\n",
    "\n",
    "# n1, bins1,_ = plt.hist(x_ones, bins=np.append(x, x[-1] + 0.05), color='r', label= \"ones\", alpha=0.5, density=True);\n",
    "\n",
    "\n",
    "def generate_x_y_gev(n0, n1, bins):\n",
    "    X = np.array([])\n",
    "    y = np.array([]);\n",
    "    binsize = bins[-1]-bins[-2];\n",
    "    for tupp in zip(n0, n1, bins ):\n",
    "#         print(tupp)\n",
    "        x_zeros = tupp[2] + binsize*np.random.rand(int(tupp[0]));\n",
    "        x_ones = tupp[2] + binsize*np.random.rand(int(tupp[1]));\n",
    "        X = np.concatenate([X, x_zeros, x_ones]);\n",
    "        y = np.concatenate([y, np.zeros(len(x_zeros)), np.ones(len(x_ones)) ]);\n",
    "#         set_trace()\n",
    "    return X,y\n",
    "\n",
    "X,y = generate_x_y_gev(n0, n1, bins1);\n",
    "plt.figure()\n",
    "X_ones = X[y==1]\n",
    "X_zeros = X[y==0]\n",
    "plt.hist(X_ones, bins=bins1, color = 'r', alpha = 0.4)\n",
    "plt.hist(X_zeros, bins=bins1, color = 'b', alpha = 0.4)\n",
    "plt.show();\n",
    "\n",
    "data_container.set_x(X.reshape(len(X), 1))\n",
    "data_container.set_y(y)\n",
    "\n",
    "print(\"X shape: {}\".format(data_container.X.shape))\n",
    "print(\"y shape: {}\".format(data_container.y.shape))\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3990000, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.reshape(len(X), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, \"Classifier's probabilities over test set\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'prob(y(x)=1)')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ROC curves')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'fpr')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'tpr')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13a42e3d0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13a51cc10>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13a42e8d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13a547810>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1206d7150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the external figure, and plot the empirical probability \n",
    "# %matplotlib qt\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_container.X, data_container.y, test_size=0.2)\n",
    "data_container.X_train = X_train\n",
    "data_container.X_test = X_test\n",
    "data_container.y_train = y_train\n",
    "data_container.y_test = y_test\n",
    "\n",
    "probs_fig, probs_axs = plt.subplots(2,1)\n",
    "data_container.probs_ax, data_container.roc_ax = probs_axs\n",
    "data_container.probs_ax.set_title(\"Classifier's probabilities over test set\")\n",
    "data_container.probs_ax.set_xlabel(\"x\")\n",
    "data_container.probs_ax.set_ylabel(\"prob(y(x)=1)\")\n",
    "\n",
    "data_container.roc_ax.set_title(\"ROC curves\")\n",
    "data_container.roc_ax.set_xlabel(\"fpr\")\n",
    "data_container.roc_ax.set_ylabel(\"tpr\")\n",
    "\n",
    "def onpick(event):\n",
    "    print(\"in onpick\")\n",
    "    # on the pick event, find the orig line corresponding to the\n",
    "    # legend proxy line, and toggle the visibility\n",
    "    # print(\"Just entered onpick!!\")\n",
    "    legline = event.artist\n",
    "    origlines = lined[legline]\n",
    "    vis = not origlines[0].get_visible()\n",
    "    for origline in origlines:\n",
    "        origline.set_visible(vis)\n",
    "    # Change the alpha on the line in the legend so we can see what lines\n",
    "    # have been toggled\n",
    "    if vis:\n",
    "        legline.set_alpha(1.0)\n",
    "    else:\n",
    "        legline.set_alpha(0.2)\n",
    "    probs_fig.canvas.draw()\n",
    "\n",
    "probs_fig.canvas.mpl_connect('pick_event', onpick)    \n",
    "data_container.probs_fig = probs_fig\n",
    "\n",
    "#Overlay the empirical probabilities (taken from the actual \"real\" distributions).\n",
    "def calc_bin_prob(bn_df):\n",
    "#     set_trace()\n",
    "    bin_x = bn_df[0].left\n",
    "    df = bn_df[1]\n",
    "    prob_1 = np.nan if len(df) == 0 else df['y'].sum()/len(df) \n",
    "    return bin_x, prob_1\n",
    "\n",
    "X = data_container.X\n",
    "y = data_container.y\n",
    "X_y_df = pd.DataFrame({'X':X.T[0], 'y':y})\n",
    "#Extend range otherwise inerval may be nan.\n",
    "bin_size = 0.05\n",
    "X_y_df['x_binned'] = pd.cut(X_y_df['X'], bins=np.arange(X.min()-bin_size, X.max() + bin_size, bin_size))\n",
    "x_binned = []\n",
    "prob_one_binned = []\n",
    "for bn_df in X_y_df.groupby('x_binned'):\n",
    "    bin_x, prob_1 = calc_bin_prob(bn_df)\n",
    "    x_binned.append(bin_x)\n",
    "    prob_one_binned.append(prob_1)\n",
    "prob_one_binned = np.array(prob_one_binned)\n",
    "data_container.probs_ax.plot(x_binned, prob_one_binned, 'r', label=\"empirical P(y=1)\")\n",
    "\n",
    "derivative = (prob_one_binned[1:]-prob_one_binned[:-1])/bin_size\n",
    "data_container.probs_ax.plot(x_binned[:-1], derivative, 'y', label=\"derivative\")\n",
    "# data_container.probs_ax.plot(x_binned, prob_one_binned/(1-prob_one_binned), 'r+', label=\"empirical P(y=1)/P(y=0)\")\n",
    "# data_container.probs_ax.plot(x_binned, np.log(prob_one_binned/(1-prob_one_binned)), 'rx', label=\"empirical log(P(y=1)/P(y=0))\")\n",
    "\n",
    "data_container.probs_ax.legend()\n",
    "#Roc\n",
    "x_test_binned = X_y_df.set_index('X').loc[X_test.T[0],'x_binned'].values\n",
    "# if np.nan in x_test_binned:\n",
    "#     print(\"Found NAN!!\")\n",
    "#     for i,intr in enumerate(x_test_binned):\n",
    "#         if isinstance(intr, float):\n",
    "#             print(f\"The nan is at index {i}, corresponding to x_test {X_test[i]}\")\n",
    "def get_left(interval):\n",
    "    return interval.left\n",
    "\n",
    "\n",
    "x_test_binned = list(map(get_left , x_test_binned))\n",
    "empirical_probs = pd.DataFrame({'probs':prob_one_binned}, index = x_binned).loc[x_test_binned]\n",
    "fpr, tpr, thresh = roc_curve(y_test, empirical_probs)\n",
    "emp_auc = roc_auc_score(y_test, empirical_probs)\n",
    "data_container.roc_ax.plot(fpr, tpr, 'r', label=\"empirical, auc:{:0.2f}\".format(emp_auc))\n",
    "data_container.roc_ax.legend()\n",
    "#TODO: calc recall, precision, F1 over the empirical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now, let's apply a standard classification and assess the results.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.87      0.87    150612\n",
      "         1.0       0.97      0.97      0.97    647388\n",
      "\n",
      "    accuracy                           0.95    798000\n",
      "   macro avg       0.92      0.92      0.92    798000\n",
      "weighted avg       0.95      0.95      0.95    798000\n",
      "\n",
      "Confusion matrix, without normalization\n",
      "[[628475  18913]\n",
      " [ 19300 131312]]\n",
      "No Skill: ROC AUC=0.500\n",
      "Logistic: ROC AUC=0.988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13475d950>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x124fcc9d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "X_train = data_container.X_train\n",
    "y_train = data_container.y_train\n",
    "X_test = data_container.X_test\n",
    "y_test = data_container.y_test\n",
    "\n",
    "\n",
    "clf_lr = LogisticRegressionCV(cv=5, random_state=4).fit(X_train, y_train)\n",
    "y_hat_lr = clf_lr.predict(X_test)\n",
    "prediction_probs = classification_assessment(X_test, y_test, y_hat_lr, clf_lr, data_container)\n",
    "# %matplotlib qt\n",
    "data_container.probs_ax.scatter(X_test[:,0], prediction_probs, s=2, label=f\"Baseline logistic\")\n",
    "\n",
    "\n",
    "data_container.probs_ax.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Next, we're going to dilute the '1' class to obtain an imbalanced dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcabcb611964531a157ff4b85e9e366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='dilute_factor', max=1.0, min=0.01, step=0.01), Buttâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "@interact_manual\n",
    "def dilute_ones( dilute_factor=widgets.FloatSlider(min=0.01,max=1,step=0.01,value=0.01)):\n",
    "    X = data_container.X\n",
    "    y = data_container.y\n",
    "    #Dilute the 1's class\n",
    "    diluted_X, diluted_y = dilute_class(X, y, 1, dilute_factor)\n",
    "    data_container.one_dilute_factor = dilute_factor\n",
    "    data_container.diluted_X = diluted_X\n",
    "    data_container.diluted_y = diluted_y\n",
    "    #Dilute the train set separately, since we want to evaluate over the original test set, \n",
    "    # so we don't want any of it to be used for training. \n",
    "    diluted_X_train, diluted_y_train = dilute_class(data_container.X_train, data_container.y_train, 1, dilute_factor)\n",
    "    data_container.diluted_X_train_ones = diluted_X_train\n",
    "    data_container.diluted_y_train_ones = diluted_y_train\n",
    "    \n",
    "    \n",
    "    diluted_X_zeros = diluted_X[diluted_y == 0]\n",
    "    diluted_X_ones = diluted_X[diluted_y == 1]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(x=diluted_X_zeros.T[0], nbinsx=1000, name=\"zeros\"))\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_ones.T[0], nbinsx=1000, name=\"ones\"))\n",
    "    fig.show()\n",
    "    \n",
    "    hist_ax = data_container.probs_ax.twinx()\n",
    "    hist_ax.hist(diluted_X_zeros.T[0], bins=1000, alpha=0.1, color='b')\n",
    "    hist_ax.hist(diluted_X_ones.T[0], bins=1000, alpha=0.1, color='r')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diluted_clf = LogisticRegressionCV(cv=5, random_state=0).fit(data_container.diluted_X, data_container.diluted_y)\n",
    "# print(\"Before correction: Theta0_D: {}, theta1_D: {}\".format(diluted_clf.intercept_, diluted_clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Now, let's examine several methods to cope with the imbalance. </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5457a9a8cda84d819b9040b0f3e11017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='zero_dilute_factor', max=1.0, min=0.01, step=0.01), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "#Predict on diluted data, optionally dilute 0 class as well.\n",
    "def generate_corrected_clf(X, y, correction_method, tau, y_bar):\n",
    "    \n",
    "    if correction_method == 'none':\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "    #Apply correction\n",
    "    if correction_method == \"prior\":\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
    "        corrected_intercept = corrected_clf.intercept_ - np.log( (1-tau)/tau*y_bar/(1-y_bar)    )\n",
    "        corrected_clf.intercept_ = corrected_intercept\n",
    "       \n",
    "    if correction_method == \"weighting\":\n",
    "        w1 = tau/y_bar\n",
    "        w0 = (1-tau)/(1-y_bar)\n",
    "        corrected_clf = LogisticRegressionCV(cv=5, random_state=0, class_weight={0:w0, 1:w1}).fit(X, y)\n",
    "    if correction_method == 'gev':\n",
    "        corrected_clf = GevRegressionCV().fit(X,y)\n",
    "    return corrected_clf\n",
    "        \n",
    "@interact_manual\n",
    "def dilute_zeros_and_predict( zero_dilute_factor=widgets.FloatSlider(min=0.01,max=1,step=0.01,value=1), \n",
    "                           correction_method = ['none', 'prior', 'weighting', 'gev']):\n",
    "    diluted_X = data_container.diluted_X\n",
    "    diluted_y = data_container.diluted_y\n",
    "    #Dilute the 0's class\n",
    "   \n",
    "    diluted_1_0_X, diluted_1_0_y = dilute_class(diluted_X, diluted_y, 0, zero_dilute_factor)\n",
    "    data_container.diluted_X_ones_and_zeros = diluted_1_0_X\n",
    "    data_container.diluted_y_ones_and_zeros = diluted_1_0_y\n",
    "    #Dilute the train set separately, since we want to evaluate over the original test set, \n",
    "    # so we don't want any of it to be used for training. \n",
    "    diluted_X_train_ones_and_zeros, diluted_y_train_ones_and_zeros =\\\n",
    "        dilute_class(data_container.diluted_X_train_ones, data_container.diluted_y_train_ones, 0, zero_dilute_factor)\n",
    "    data_container.diluted_X_train_ones_and_zeros = diluted_X_train_ones_and_zeros\n",
    "    data_container.diluted_y_train_ones_and_zeros = diluted_y_train_ones_and_zeros\n",
    "    \n",
    "    \n",
    "    \n",
    "    diluted_X_zeros = diluted_1_0_X[diluted_1_0_y == 0]\n",
    "    diluted_X_ones = diluted_1_0_X[diluted_1_0_y == 1]\n",
    "    y = data_container.y\n",
    "    tau = sum(y)/len(y) #The actual population ratio, before any dilution.\n",
    "    y_bar =  sum(diluted_1_0_y)/len(diluted_1_0_y) #The sample ratio.\n",
    "    #Predict\n",
    "    display(HTML(f'<h3>Using a dataset of size {len(diluted_1_0_y)}, {len(diluted_X_zeros)} of which are 0,\\\n",
    "    and {len(diluted_X_ones)} are 1 <h3>'))\n",
    "    print(\"Before performing classification over the diluted data.\")\n",
    "    print(\"Original dataset: zeros: {}, ones: {}\".format(sum(data_container.y==0), sum(data_container.y == 1)))\n",
    "    print(\"Diluted training dataset: zeros: {}, ones: {}\".format(sum(data_container.diluted_y_train_ones_and_zeros==0), \n",
    "                                                        sum(data_container.diluted_y_train_ones_and_zeros == 1)))\n",
    "    \n",
    "    corrected_clf = generate_corrected_clf(data_container.diluted_X_train_ones_and_zeros, \n",
    "                                           data_container.diluted_y_train_ones_and_zeros, \n",
    "                                           correction_method, tau, y_bar)\n",
    "    X_test = data_container.X_test\n",
    "    y_test = data_container.y_test\n",
    "    y_hat = corrected_clf.predict(X_test)\n",
    "    prediction_probs = classification_assessment(X_test, data_container.y_test, \n",
    "                                                 y_hat, corrected_clf, data_container)\n",
    "#     print(prediction_probs)\n",
    "#     print(prediction_probs.shape)\n",
    "#     print(sum(prediction_probs > 0.5))\n",
    "#     probs = corrected_clf.predict_proba(X_test)\n",
    "    data_container.probs = prediction_probs\n",
    "#     print(probs)\n",
    "#     print(\"######################{}###\".format(any(probs[:,1] != prediction_probs )))\n",
    "    data_container.probs_ax.scatter(X_test[:,0], prediction_probs, s=2, \n",
    "                                    label=\"0:{}%_1:{}%_corr:{}\".format(\n",
    "                                    int(zero_dilute_factor*100), int(data_container.one_dilute_factor*100), \n",
    "                                        correction_method))\n",
    "    data_container.probs_ax.legend()\n",
    "#     lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, probs[:,1])\n",
    "#     print(sum(probs[:,1]>0.5))\n",
    "#     print(lr_tpr)\n",
    "#     data_container.roc_ax.scatter(lr_fpr, lr_tpr)\n",
    "# #     probs_fig = go.Figure()\n",
    "#     probs_fig.add_trace(go.Scatter( x= X_test.T, y=probs[:,1].T ))\n",
    "#     probs_fig.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    fig = go.Figure();\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_zeros.T[0], nbinsx=1000))\n",
    "    fig.add_trace(go.Histogram(x=diluted_X_ones.T[0], nbinsx=1000))\n",
    "    fig.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "X_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)\n",
    "probs = np.array(0.5*np.random.rand(len(y_test)))\n",
    "sum(probs>0.5)\n",
    "lr_fpr, lr_tpr, lr_thresh = roc_curve(y_test, probs)\n",
    "print(lr_thresh)\n",
    "plt.plot(lr_fpr, lr_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diluted_X = data_container.diluted_X\n",
    "diluted_y = data_container.diluted_y\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5, random_state=0).fit(diluted_X, diluted_y)\n",
    "y_hat_tmp = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "print(sum(probs>0.5))\n",
    "fpr, tpr, thresh = roc_curve(y_test, y_hat_tmp)\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## GEV #######################\n",
    "# import scipy.optimize.fmin_cg\n",
    "def gev(x, tau):\n",
    "    return np.exp( -np.power(1 + tau*x, -1/tau))\n",
    "\n",
    "\n",
    "class GevRegressionCV():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.curr_iter_x_dot_theta = None\n",
    "        self.curr_iter_pi = None\n",
    "    \n",
    "    @staticmethod \n",
    "    def err_func(theta_tau, X, y, reg_param, gev_instance):\n",
    "        x_dot_theta = np.dot(X, theta)\n",
    "        pi = gev(x_dot_theta)\n",
    "        h_theta = -1/len(y) *( np.dot(y, np.log(pi)) + np.dot(1-y, np.log(1 - pi))) + \n",
    "            reg_param/(2*len(y))*np.dot(theta_tau, theta_tau)\n",
    "        #Save for the gradient invocation, to save repeated evaluation. \n",
    "#         gev_instance.curr_iter_x_dot_theta =  x_dot_theta\n",
    "#         gev_instance.curr_iter_pi = pi\n",
    "        print(f\"h_theta: {h_theta}\")\n",
    "        return h_theta\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad(theta_tau, X, y, reg_param, gev_instance):\n",
    "        #These two are calculated twice. Make this more efficient later if necessary. \n",
    "        x_dot_theta = np.dot(X, theta)\n",
    "        pi = gev(x_dot_theta)\n",
    "#         x_dot_theta = gev_instance.curr_iter_x_dot_theta\n",
    "#         pi = gev_instance.curr_iter_pi\n",
    "        mult_vec = np.log(pi)*(y-pi)/( (1+tau*x_dot_theta)*(1-pi) )\n",
    "        grad_theta = -np.dot(X.T, mult_vec)\n",
    "        #Regularization\n",
    "        grad_theta[1:] += reg_param/len(y)*theta_tau[1:]\n",
    "        \n",
    "        u = 1/(tau*tau)*np.log((1+tau*x_dot_theta)) - x_dot_theta/( tau*(1+tau*x_dot_theta))\n",
    "        v = (y-pi)*np.log(pi)/(1-pi)\n",
    "        grad_tau = -np.dot(u,v) #TODO: regularize tau as well?\n",
    "        \n",
    "        return h_theta, np.append([grad_theta, grad_tau])\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        #This function requires two separate functions calls - one for the log-likelihood, \n",
    "        # and the other for the gradient calculation. I'm saving the \n",
    "        #Add intercept\n",
    "        first_col = np.array([np.ones(X.shape[0])])\n",
    "        X = np.concatenate((b.T, X), axis=1)\n",
    "        initial_guess\n",
    "        opt_theta_tau = scipy.optimize.fmin_cg(self.err_func, initial_guess,\n",
    "                                               fprime=self.grad, args=(X, y, self) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import genextreme\n",
    "r = genextreme.rvs(c=-0.0000000007, loc=0, scale=100, size=100000)\n",
    "fig,ax = plt.subplots()\n",
    "n, bins, _ = ax.hist(r, bins = 10000, density=True)\n",
    "x_0 = []\n",
    "x_1 = []\n",
    "bin_size = bins[1] - bins[0]\n",
    "rands = np.random.rand(len(n))\n",
    "for dens,bn,rnd in zip(n,bins,rands):\n",
    "    arr_to_add = x_1 if rnd < dens*bin_size else x_0\n",
    "    arr_to_add.append(bn)\n",
    "ax.hist(x_0, bins=10000, density=True, color='b', label='zeros')\n",
    "ax.hist(x_1, bins=10000, density=True, color='r', label='ones')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, _ = plt.hist([1,2,3,4,5], bins = 4, density=True)\n",
    "n\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.random.rand(100000)\n",
    "lmbd=1\n",
    "k=1.5\n",
    "weibull = lmbd*np.power((-np.log(1-u)),1/k) \n",
    "# plt.hist(weibull, bins = 1000, density=True)\n",
    "fig,ax = plt.subplots()\n",
    "n, bins, _ = ax.hist(weibull, bins = 1000, density=True)\n",
    "x_0 = []\n",
    "x_1 = []\n",
    "bin_size = bins[1] - bins[0]\n",
    "rands = np.random.rand(len(n))\n",
    "for dens,bn,rnd in zip(n,bins,rands):\n",
    "    arr_to_add = x_1 if rnd < dens*bin_size else x_0\n",
    "    arr_to_add.append(bn)\n",
    "# ax.hist(x_0, bins=1000, density=True, color='b', label='zeros')\n",
    "# ax.hist(x_1, bins=1000, density=True, color='r', label='ones')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expn = np.random.exponential(100, 100000)\n",
    "plt.hist(expn, bins=1000, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 blobs test\n",
    "num_samples = 200000\n",
    "center_zero = 0\n",
    "center_one_1 = 2\n",
    "center_one_2 = -3\n",
    "std_zero = 0.7\n",
    "std_one = 0.6\n",
    "    \n",
    "dataset1 =  make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one_1], \n",
    "                                                                                     [center_one_2]],\n",
    "                                cluster_std=[std_zero, std_one, std_one],  shuffle=False, random_state=4) \n",
    "     \n",
    "# dataset1 =  make_blobs(n_samples=num_samples, n_features=1, centers=[[center_zero], [center_one_1]],\n",
    "#                             cluster_std=[std_zero, std_one],  shuffle=False, random_state=4) \n",
    "X, y = dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y==2]=1\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = X[y==1]\n",
    "X_zeros = X[y==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots()\n",
    "bins = np.arange(-5,4,0.05);\n",
    "ax.hist(X_ones, bins=bins, label='ones', alpha=.3, density=True)\n",
    "ax.hist(X_zeros, bins=bins, label='zeros', alpha=0.3, density=True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "XX = np.concatenate([X, X*X], axis=1)\n",
    "XX.shape\n",
    "X[0]*X[0] == XX[0,1]\n",
    "X[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(XX, y, test_size=0.2)\n",
    "clf = LogisticRegressionCV(cv=5, random_state=4).fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "probs = clf.predict_proba(X_test)[:,1]\n",
    "probs.shape\n",
    "X_test.shape\n",
    "ax.scatter(X_test[:,0], probs, color='r')\n",
    "\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diluted_X, diluted_y = dilute_class(X, y, 1, 0.1)\n",
    "diluted_X, diluted_y = dilute_class(diluted_X, diluted_y, 0, 0.1)\n",
    "plt.scatter(diluted_X[:,0], diluted_y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([clf.intercept_, clf.coef_])\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ones = np.concatenate([np.ones(len(X_test)), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(np.arange(-2,2,0.05))\n",
    "y = np.log(1+np.exp(2*x))\n",
    "plt.plot(x,y, x , np.log(np.exp(2*x           )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fpr, lr_tpr, lr_thresh = roc_curve(data_container.y_test, data_container.probs)\n",
    "plt.figure()\n",
    "# plt.scatter(data_container.X_test, data_container.probs)\n",
    "plt.plot(lr_fpr, lr_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_container.X_test, data_container.y_test)\n",
    "plt.scatter(data_container.X_test, data_container.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lr_tpr)\n",
    "#plt.scatter(data_container.X_test, lr_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(lr_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "precision, recall, thresholds = precision_recall_curve( data_container.y_test, data_container.probs)\n",
    "plt.plot(  recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(thresholds)\n",
    "len(np.unique(data_container.probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([1,2,3],[4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Weibull</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3990000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in power\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu = -2\n",
    "sigma = 2\n",
    "k = 1.5\n",
    "x = np.arange(mu+0.05, mu+10*sigma, 0.05);\n",
    "F = 1-np.exp(-np.power((-(mu-x)/sigma), k));\n",
    "num_repetitions = 10000\n",
    "xx = np.array([x]*num_repetitions);\n",
    "xx = xx.reshape(1, num_repetitions*len(x))[0];\n",
    "\n",
    "u1 = np.random.rand(len(xx));#\n",
    "# u2 = np.random.rand(len(x))\n",
    "F_inverse_u = xx - sigma*np.power((-np.log(1-u1)), 1/k);#\n",
    "\n",
    "print(F_inverse_u.shape);\n",
    "# F_inverse_u\n",
    "x_ones = xx[F_inverse_u >= 0];\n",
    "x_zeros = xx[F_inverse_u < 0];\n",
    "n1, bins1,_ = plt.hist(x_ones, bins=np.append(x, x[-1] + 0.05), color='r', label= \"ones\", alpha=0.5, density=False);\n",
    "n0, bins0,_ = plt.hist(x_zeros, bins=np.append(x, x[-1] + 0.05), color='b', label= \"zeros\", alpha=0.5, density=False);\n",
    "empirical_probs = list(map(lambda tup : tup[0]/(tup[0]+tup[1]), zip(n1,n0) ))\n",
    "\n",
    "F_0 = 1-np.exp(-np.power((-(-x)/sigma), k));\n",
    "plt.plot(x, F_0, color='y');\n",
    "plt.plot(x, empirical_probs, color='g');\n",
    "F_sig = 1/(1+np.exp(-x/(0.5*sigma)))\n",
    "plt.plot(x, F_sig, color='k');\n",
    "plt.legend();\n",
    "# plt.plot(x, F)\n",
    "\n",
    "\n",
    "# n1, bins1,_ = plt.hist(x_ones, bins=np.append(x, x[-1] + 0.05), color='r', label= \"ones\", alpha=0.5, density=True);\n",
    "\n",
    "\n",
    "def generate_x_y_gev(n0, n1, bins):\n",
    "    X = np.array([])\n",
    "    y = np.array([])\n",
    "    binsize = bins[-1]-bins[-2]\n",
    "    for tupp in zip(n0, n1, bins ):\n",
    "#         print(tupp)\n",
    "        x_zeros = tupp[2] + binsize*np.random.rand(int(tupp[0]))\n",
    "        x_ones = tupp[2] + binsize*np.random.rand(int(tupp[1]))\n",
    "        X = np.concatenate([X, x_zeros, x_ones])\n",
    "        y = np.concatenate([y, np.zeros(len(x_zeros)), np.ones(len(x_ones)) ])\n",
    "#         set_trace()\n",
    "    return X,y\n",
    "\n",
    "X,y = generate_x_y_gev(n0, n1, bins1);\n",
    "plt.figure()\n",
    "X_ones = X[y==1]\n",
    "X_zeros = X[y==0]\n",
    "plt.hist(X_ones, bins=bins1, color = 'r', alpha = 0.4)\n",
    "plt.hist(X_zeros, bins=bins1, color = 'b', alpha = 0.4)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1])\n",
    "b = np.array([2])\n",
    "c = [3]\n",
    "d = np.concatenate([a,b,c])\n",
    "np.random.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = np.random.rand(2,3)\n",
    "x = np.array([1,2]*3)\n",
    "# tmp = x + np.exp(rn)\n",
    "rn\n",
    "x\n",
    "np.exp(rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append([np.array([]), np.arange(1,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
